{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d056aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob,pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import git\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm,trange\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import WordNetError\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate#, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e880d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b5dff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_parse(_fpath):\n",
    "\n",
    "    sctree = ET.parse(_fpath)\n",
    "\n",
    "    # Iterates over list of words in files    \n",
    "    dct_list1 = []\n",
    "    for node in sctree.iter('wf'):\n",
    "        attributes = node.attrib\n",
    "        attributes['text'] = node.text\n",
    "        dct_list1.append(attributes)\n",
    "\n",
    "    # Iterates over terms to find senses and corresponding sense references\n",
    "    dct_list2 = []\n",
    "    for term in sctree.iter('term'):\n",
    "        lemma = term.attrib.get('lemma')\n",
    "        wordid = term.find('span/target').attrib.get('id')\n",
    "        pos = ''\n",
    "\n",
    "        wnsn = '0'\n",
    "        senseid=''\n",
    "        if term.findall('externalReferences/externalRef'):\n",
    "            wnsn = term.findall('externalReferences/externalRef')[0].attrib.get('reference')\n",
    "            senseid = term.findall('externalReferences/externalRef')[1].attrib.get('reference')\n",
    "        dct_list2.append({'id':wordid,'lemma':lemma,'wn_sense_num':wnsn,'lexical_key':senseid,'pos':term.attrib['pos']})\n",
    "\n",
    "    word_df = pd.DataFrame(dct_list1)\n",
    "    sense_ref_df = pd.DataFrame(dct_list2)   \n",
    "    \n",
    "    return pd.merge(word_df,sense_ref_df,on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07611370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_file_list(_basepath,ext='*.naf'):\n",
    "    \n",
    "    file_list = []\n",
    "    fla = glob.glob(os.path.join(_basepath,ext))\n",
    "    flb = glob.glob(os.path.join(_basepath,'*',ext))\n",
    "    flc = glob.glob(os.path.join(_basepath,'**',ext))\n",
    "    files = set(fla+flb+flc)\n",
    "    for fileref in files: #search recursively for files\n",
    "        parent_folder_name = pathlib.Path(fileref).parent.name\n",
    "        file_name = pathlib.Path(fileref).name.split('.')[0]\n",
    "        \n",
    "        file_list.append( {'file_path':fileref,\n",
    "                           'parent_folder':parent_folder_name,\n",
    "                           'file_name':file_name})\n",
    "    return pd.DataFrame(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4decbe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_corpus(_basepath,filter_validation = False):\n",
    "\n",
    "   # generate dataframe with references to all files\n",
    "    _fpath_df = gen_file_list(_basepath)\n",
    "    \n",
    "    # filter to remove validation files\n",
    "    filtered_file_df = _fpath_df\n",
    "    if filter_validation:\n",
    "         filtered_file_df = _fpath_df[_fpath_df.parent_folder != 'brownv']\n",
    "    \n",
    "    _dflist = []\n",
    "    for i,file_entry in tqdm(filtered_file_df.iterrows(), total=filtered_file_df.shape[0]):\n",
    "        _parsed_file_df = xml_parse(file_entry.file_path)\n",
    "        #print(_parsed_file_df.head())\n",
    "        _parsed_file_df['file'] = file_entry.file_name\n",
    "        _dflist.append(_parsed_file_df)\n",
    "\n",
    "    return pd.concat(_dflist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eeba6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(_basepath,verbose=True,**kwargs):\n",
    "    if verbose: print('Parsing corpus')\n",
    "    base_corpus = parse_corpus(_basepath,**kwargs)\n",
    "\n",
    "    # Build wordnet ref key using wordnet lemma\n",
    "    if verbose: print('Preprocessing indexes...',end=\"\")\n",
    "    base_corpus['wn_index'] = base_corpus['lemma']+'%'+base_corpus['lexical_key']\n",
    "\n",
    "    base_corpus.loc[base_corpus.lexical_key == '','wn_index'] = ''\n",
    "    base_corpus.drop('lexical_key',axis=1,inplace=True)\n",
    "    if verbose: print('Done!')\n",
    "    print(base_corpus.shape)\n",
    "    return base_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f56ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_get_glosses(_word,_sense_id):\n",
    "    _sense_id = int(_sense_id)\n",
    "    if not _word: # if ref is empty\n",
    "        return ''\n",
    "    try:\n",
    "        all_synsets = wn.synsets(_word)\n",
    "        target_gloss = []\n",
    "        other_glosses = []\n",
    "        for syn in all_synsets:\n",
    "            split = syn.name().split('.')\n",
    "            wn_lemma = split[0]\n",
    "            sense_num = int(split[-1])\n",
    "            #if _word == wn_lemma:    \n",
    "            if sense_num == _sense_id:\n",
    "                target_gloss.append(syn.definition()) \n",
    "            else:\n",
    "                other_glosses.append(syn.definition())                \n",
    "        return target_gloss,other_glosses\n",
    "    except (AttributeError,WordNetError,ValueError) as err:\n",
    "        return 'WN Error',None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d5d43ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_gloss_helper(_word,_sense_id):\n",
    "    if not _word or not _sense_id:\n",
    "        return '',''\n",
    "    senseidlist = _sense_id.split(';')\n",
    "    if len(senseidlist) == 1:\n",
    "        return wordnet_get_glosses(_word,int(_sense_id))\n",
    "    elif len(senseidlist) > 1:\n",
    "        list_proper_glosses = []\n",
    "        other_gloss_set = set()\n",
    "        for senseid in senseidlist:\n",
    "            gloss, other_glosses =  wordnet_get_glosses(_word,int(senseid))\n",
    "            if gloss:\n",
    "                list_proper_glosses.append(gloss)\n",
    "                other_gloss_set.update(set(other_glosses))\n",
    "        # if one of the glosses is bogus return only one\n",
    "        if len(list_proper_glosses) == 1:\n",
    "            return list_proper_glosses[0], other_gloss_set\n",
    "        #print(list_proper_glosses, other_gloss_set)\n",
    "        return list_proper_glosses, other_gloss_set\n",
    "    else:\n",
    "        return  'WN Error',[]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa47ed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wordnet_gloss(_semcordf,verbose=True):\n",
    "\n",
    "    if verbose: print('Adding wordnet glosses')\n",
    "    _semcordf['idx'] = list(range(len(_semcordf))) #adding index for merging\n",
    "    tqdm.pandas(desc=\"Gloss preprocessing\") \n",
    "    _glosses = _semcordf[_semcordf.wn_sense_num != '0'].progress_apply(lambda _row: (*wordnet_gloss_helper(_row['lemma'],_row['wn_sense_num'])\\\n",
    "                                                                        ,_row['idx']),axis=1 )\n",
    "    _df_glosses = pd.DataFrame(_glosses.values.tolist(),columns=['gloss','other_glosses','idx'])\n",
    "    _merged = pd.merge(_semcordf,_df_glosses,on='idx',how='left').fillna('')\n",
    "    # for now take only first gloss\n",
    "    _merged['gloss'] = _merged.gloss.apply(lambda x: x[0] if x else '')\n",
    "    # tag how many other glosses there are\n",
    "    _merged['other_glossesnum'] = _merged.other_glosses.apply(lambda x: len(x))   \n",
    "    if verbose: print('Done!')   \n",
    "    return _merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f94b553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sentence_context_pairs(_df):\n",
    "    sentence = _df.text.str.cat(sep = ' ').replace(\" '\",\"'\")#_df.text.tolist()\n",
    "    basedct = {'context':sentence,\n",
    "               'file':_df.iloc[0].file}\n",
    "\n",
    "    semcor_sentences = []\n",
    "\n",
    "    # Make sure there are other glosses and that the gloss column is not null\n",
    "    for i,(j,line) in enumerate(_df[(_df.other_glossesnum > 0) & (_df.gloss != 'WN Error') & (_df.gloss != '')].iterrows()): \n",
    "\n",
    "        newbasedct = basedct.copy()\n",
    "        #newbasedct['context'] = ' '.join(sentence[max(0,i-3): min(i+3,len(sentence))])\n",
    "        newbasedct['target_word'] = line.text\n",
    "        newbasedct['gloss'] = line.gloss\n",
    "        newbasedct['is_proper_gloss'] = True\n",
    "        semcor_sentences.append(newbasedct)\n",
    "        # Then append all different contexes with False labels\n",
    "        for other_glosses in line.other_glosses:\n",
    "            newbasedct = basedct.copy()\n",
    "            #newbasedct['context'] = ' '.join(sentence[max(0,i-3): min(i+3,len(sentence))])\n",
    "            newbasedct['target_word'] = line.text\n",
    "            newbasedct['gloss'] = other_glosses\n",
    "            newbasedct['is_proper_gloss'] = False\n",
    "            semcor_sentences.append(newbasedct)\n",
    "                \n",
    "    return semcor_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "204a6479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_joint_dataset(_df):\n",
    "    groupbyobj = _df.groupby(['sent','file'])\n",
    "    full_dict_list = []\n",
    "    for [sentnum,file],gp in tqdm(groupbyobj,total=len(groupbyobj)):\n",
    "        full_dict_list.extend(gen_sentence_context_pairs(gp))\n",
    "    cols = ['file','context','target_word','gloss','is_proper_gloss']\n",
    "    return pd.DataFrame(full_dict_list)[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7fc06e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus_dataset(_basepath,verbose=True,byref=False):\n",
    "    \n",
    "    corpus_df = build_corpus(_basepath,verbose=verbose)\n",
    "    corpus_df = add_wordnet_gloss(corpus_df,verbose=verbose)\n",
    "    if verbose: print('Processing adn labeling joint context-gloss pairs...',end=\"\")\n",
    "    final_corpus = build_joint_dataset(corpus_df)\n",
    "    if verbose: print('Done!')    \n",
    "    return final_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55015e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 352/352 [02:54<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing indexes...Done!\n",
      "(676546, 10)\n",
      "Adding wordnet glosses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gloss preprocessing: 100%|███████████████████████████████████████████████████| 226040/226040 [02:44<00:00, 1371.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing adn labeling joint context-gloss pairs..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████████████████████████████████████████████████████████████████▎      | 33917/37168 [11:27<00:53, 60.36it/s]"
     ]
    }
   ],
   "source": [
    "fpath = \"./data/raw/semcor3.0/\"\n",
    "savepath = r\"./data/preprocessed/semcor_gloss.pkl\"\n",
    "final_corpus = build_corpus_dataset(fpath)\n",
    "final_corpus.to_pickle(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c34b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus = pd.read_pickle(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7fee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_final_corpus = final_corpus\n",
    "orig_final_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93214f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus[final_corpus.is_proper_gloss==True].shape,final_corpus[final_corpus.is_proper_gloss==False].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df70016",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df, y_df  = final_corpus.iloc[:, :-1] , final_corpus.iloc[:, -1]\n",
    "\n",
    "under = RandomUnderSampler(sampling_strategy=0.4)\n",
    "X_df, y_df = under.fit_resample(X_df, y_df)\n",
    "\n",
    "over = RandomOverSampler(sampling_strategy=.5)\n",
    "X_df,y_df = over.fit_resample(X_df , y_df)\n",
    "\n",
    "X_df['is_proper_gloss'] = y_df\n",
    "final_corpus = X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f889b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus[final_corpus.is_proper_gloss==True].shape,final_corpus[final_corpus.is_proper_gloss==False].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55142062",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus = final_corpus.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df =  train_test_split(final_corpus, \n",
    "                                        random_state=None, \n",
    "                                        test_size=.1)\n",
    "val_df, test_df =  train_test_split(val_df, \n",
    "                                        random_state=None, \n",
    "                                        test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ac44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape , val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d435d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLDataGen(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, df, batch_size = 64, gen_type=  'train', model_file_lang1 = None, model_file_lang2 = None):\n",
    "        self.context =list(map(lambda row:  ''.join(char for char in row[2] if char.isalnum())+' '+row[1]+' '+ ''.join(char for char in row[2] if char.isalnum()),df.to_numpy())) \n",
    "        self.tagged_sense = list(map(lambda row: str(row[3]),df.to_numpy())) \n",
    "        self.label= df['is_proper_gloss'].astype(int).to_numpy()\n",
    "        self.gen_type = gen_type\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.max_seq_len=200\n",
    "        self.x0_vocab_size =0\n",
    "        self.x1_vocab_size =0\n",
    "        self.x0_tokenizer = None\n",
    "        self.x1_tokenizer = None\n",
    "        self._x0=[]\n",
    "        self._x1=[]\n",
    "        self._y=[]\n",
    "        self._initialize_variables()\n",
    "        self.model_file_lang1 = model_file_lang1\n",
    "        self.model_file_lang2 = model_file_lang2\n",
    "        #oov_tok = '<OOV>'\n",
    "\n",
    "\n",
    "    def _initialize_variables(self):         \n",
    "        \n",
    "        if self.gen_type == 'train':\n",
    "            tokenized = [line.split() for line in self.context]\n",
    "            flattendata = list(itertools.chain.from_iterable(tokenized))\n",
    "            word_freq = dict(Counter(flattendata))\n",
    "            tot_cnt  = len(word_freq)\n",
    "            #cnt = len([k for k,v in word_freq.items() if v >= 1])\n",
    "\n",
    "            # Prepare a tokenizer, again -- by not considering the rare words\n",
    "            self.x0_tokenizer = Tokenizer(num_words = tot_cnt+1 ,oov_token= '<OOV>')\n",
    "            # saving\n",
    "            with open('tokenizer_lang1.pickle', 'wb') as handle:\n",
    "                pickle.dump(self.x0_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        else:\n",
    "            #if self.model_file_lang1 != None: print('Please provide tokenizer model for lang1.')\n",
    "\n",
    "            # loading\n",
    "            with open('tokenizer_lang1.pickle', 'rb') as handle:\n",
    "                self.x0_tokenizer = pickle.load(handle)\n",
    "                \n",
    "            if self.x0_tokenizer  ==None:\n",
    "                print('Vocabulary is not set. Try training mode')\n",
    "                \n",
    "        self.x0_tokenizer.fit_on_texts(list(self.context))\n",
    "        self.x0_vocab_size = self.x0_tokenizer.num_words + 1\n",
    "        \n",
    "        # Convert text sequences to integer sequences \n",
    "        x0_train_seq = self.x0_tokenizer.texts_to_sequences(self.context) \n",
    "\n",
    "        # Pad zero upto maximum length\n",
    "        x0_train = pad_sequences(x0_train_seq,  maxlen=self.max_seq_len, padding='post')\n",
    "        \n",
    "        if self.gen_type == 'train':\n",
    "            tokenized = [line.strip().split() for line in self.tagged_sense]\n",
    "            flattendata = list(itertools.chain.from_iterable(tokenized))\n",
    "            word_freq = dict(Counter(flattendata))\n",
    "            tot_cnt  = len(word_freq)\n",
    "            #cnt = len([k for k,v in word_freq.items() if v >= 5])\n",
    "            # Prepare a tokenizer, again -- by not considering the rare words\n",
    "            self.x1_tokenizer = Tokenizer(num_words = tot_cnt ,oov_token= '<OOV>')\n",
    "            # saving\n",
    "            with open('tokenizer_lang2.pickle', 'wb') as handle:\n",
    "                pickle.dump(self.x1_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        else:\n",
    "            \n",
    "            #if self.model_file_lang2 != None: print('Please provide tokenizer model for lang2.')\n",
    "\n",
    "            # loading\n",
    "            with open('tokenizer_lang2.pickle', 'rb') as handle:\n",
    "                self.x1_tokenizer = pickle.load(handle)\n",
    "                \n",
    "            if self.x1_tokenizer  ==None:\n",
    "                print('Vocabulary is not set. Try training mode')\n",
    "\n",
    "\n",
    " \n",
    "        self.x1_tokenizer.fit_on_texts(list(self.tagged_sense))\n",
    "        self.x1_vocab_size = self.x1_tokenizer.num_words + 1\n",
    "        \n",
    "        # Convert text sequences to integer sequences \n",
    "        x1_train_seq = self.x1_tokenizer.texts_to_sequences(self.tagged_sense)\n",
    "\n",
    "        # Pad zero upto maximum length\n",
    "        x1_train = pad_sequences(x1_train_seq,  maxlen=self.max_seq_len, padding='post')\n",
    "\n",
    "        self._x0 = x0_train\n",
    "        self._x1 = x1_train\n",
    "        self._y = self.label\n",
    "    \n",
    "    \n",
    "        \n",
    "    def get_data_batch(self,i):           \n",
    "       \n",
    "        x0 = self._x0[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "        x1 = self._x1[i * self.batch_size:(i + 1) * self.batch_size]       \n",
    "        y = self._y[i * self.batch_size:(i + 1) * self.batch_size].reshape(-1,1)#y.reshape(y.shape[0], y.shape[1], 1)[:, 1:]\n",
    "        return [np.array(x0), np.array(x1)],np.array(y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X, y = self.get_data_batch(index)\n",
    "        return X,y\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self._x0) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = MLDataGen(train_df,batch_size=256)\n",
    "val_gen = MLDataGen(val_df, gen_type='val',batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f86a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = val_gen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5f3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen.context[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc6b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen._x0[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbba5e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove_file = './data/glove.6B.100d.txt'\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206892de",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_0 = data_gen.x0_tokenizer.word_index\n",
    "word_index_1 = data_gen.x1_tokenizer.word_index\n",
    "\n",
    "num_tokens_0 = (data_gen.x0_vocab_size) + 2\n",
    "num_tokens_1 = (data_gen.x1_vocab_size) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix_0 = np.zeros((num_tokens_0, embedding_dim))\n",
    "for word, i in word_index_0.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix_0[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb73d262",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_1 = np.zeros((num_tokens_1, embedding_dim))\n",
    "for word, i in word_index_1.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix_1[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8455f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64\n",
    "embedding_dim = 100\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(data_gen.max_seq_len, ))\n",
    "\n",
    "# Embedding layer\n",
    "enc_emb = Embedding(num_tokens_0 , \n",
    "                    embedding_dim, \n",
    "                    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix_0),\n",
    "                    trainable=False,\n",
    "                    mask_zero=True)(encoder_inputs)\n",
    "\n",
    "# Encoder LSTM 1\n",
    "encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.1,recurrent_dropout=0.1)\n",
    "(encoder_output1, state_h, state_c) = encoder_lstm1(enc_emb)\n",
    "\n",
    "# Set up the decoder, using encoder_states as the initial state\n",
    "decoder_inputs = Input(shape=(None, ))\n",
    "\n",
    "# Embedding layer\n",
    "dec_emb_layer = Embedding(num_tokens_1 , \n",
    "                          embedding_dim, \n",
    "                          embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix_1),\n",
    "                          trainable=False,\n",
    "                          mask_zero=True)\n",
    "\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# Decoder LSTM\n",
    "#decoder_lstm = LSTM(latent_dim, return_sequences=True,return_state=True, dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_lstm = LSTM(latent_dim, dropout=0.1,recurrent_dropout=0.1)\n",
    "#decoder_outputs, decoder_fwd_state, decoder_back_state) = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "decoder_outputs = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "#decoder_dense = TimeDistributed(Dense(data_gen.y_vocab_size , activation='softmax'))\n",
    "decoder_dense1 = Dense(64, activation='relu')#Dense(2, activation='softmax')\n",
    "decoder_outputs = decoder_dense1(decoder_outputs)\n",
    "decoder_dense2 = Dense(1, activation='sigmoid')\n",
    "decoder_outputs = decoder_dense2(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fdf492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7747654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def get_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    \n",
    "    #return true_positives/(possible_positives+K.epsilon()) \n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e200365",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=0.01)\n",
    "model.compile(#optimizer='adam', loss='categorical_crossentropy')\n",
    "                #loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "                loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "                optimizer=adam, \n",
    "                metrics=[get_score,'accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "\n",
    "checkpoint_path = \"./data/preprocessed/checkpoints/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "   checkpoint_path, verbose=1, save_weights_only=True,\n",
    "   # Save weights, every epoch.\n",
    "   save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb29e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    data_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,\n",
    "    callbacks=[es,cp_callback],\n",
    "    verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b89583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf1f585",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index = data_gen.x0_tokenizer.index_word\n",
    "reverse_source_word_index = data_gen.x0_tokenizer.index_word\n",
    "target_word_index = data_gen.x1_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc838b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Models\n",
    "\n",
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_output1,\n",
    "                      state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim, ))\n",
    "decoder_state_input_c = Input(shape=(latent_dim, ))\n",
    "decoder_hidden_state_input = Input(shape=(data_gen.max_seq_len, latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2 = decoder_lstm(dec_emb2,\n",
    "        initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense1(decoder_outputs2)\n",
    "decoder_outputs2 = decoder_dense2(decoder_outputs2)\n",
    "\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,\n",
    "                      decoder_state_input_h, decoder_state_input_c],\n",
    "                      [decoder_outputs2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7bac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x0_test_seq = data_gen.x0_tokenizer.texts_to_sequences([s+' '+target]*5)\n",
    "#x0_test = pad_sequences(x0_test_seq,  maxlen=data_gen.max_seq_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1aa386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eaf308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wn.ADJ,\n",
    "                \"N\": wn.NOUN,\n",
    "                \"V\": wn.VERB,\n",
    "                \"R\": wn.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wn.NOUN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef6fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ftestpath = './data/raw/semeval2007_task17_allwords'\n",
    "#test_corpus = build_joint_senseval_gloss_corpus(ftestpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc751aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emma = lemmatizer.lemmatize('sign_up', get_wordnet_pos('sign_up'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e490711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wn.synsets(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f76d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSense(sentence, target):\n",
    "    targets=[]\n",
    "    if '_' in target:\n",
    "        targets = target.split('_')\n",
    "    else:\n",
    "        targets=[target]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for t in targets:\n",
    "        lemmas.append(lemmatizer.lemmatize(t, get_wordnet_pos(t)))\n",
    "    lemma = '_'.join(lemmas)\n",
    "    all_synsets = wn.synsets(lemma)\n",
    "    glosses = []\n",
    "    for syn in all_synsets:\n",
    "        glosses.append(syn.definition())\n",
    "    \n",
    "    if len(glosses)==0:\n",
    "        return ''    \n",
    "    \n",
    "    x0_test_seq = data_gen.x0_tokenizer.texts_to_sequences([sentence+' '+target]*len(glosses))\n",
    "    x0_test = pad_sequences(x0_test_seq,  maxlen=data_gen.max_seq_len, padding='post')\n",
    "\n",
    "    x1_test_seq = data_gen.x1_tokenizer.texts_to_sequences(glosses)\n",
    "    x1_test = pad_sequences(x1_test_seq,  maxlen=data_gen.max_seq_len, padding='post')\n",
    "    \n",
    "    #return x0_test , x1_test    \n",
    "    try:\n",
    "        (e_out, e_h, e_c) = encoder_model.predict(x0_test)\n",
    "\n",
    "        ypred = decoder_model.predict([x1_test] + [e_out, e_h, e_c])\n",
    "    except:\n",
    "        print(lemma,targets)\n",
    "    return glosses[np.argmax(ypred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db85136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_corpus[test_corpus.target_word =='signed_up']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951592fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pred_dataset(_df):\n",
    "    context_target = _df[['context','target_word']].drop_duplicates()\n",
    "    print(context_target.shape,_df.shape)\n",
    "    basedct = {}\n",
    "    predictions = []\n",
    "    context_target['predicted_gloss'] = context_target[['context','target_word']].apply(lambda row: predictSense(row[0],row[1]),axis=1)\n",
    "    prediction_df = pd.merge(_df,context_target,on=['context','target_word'],how='right').fillna('')\n",
    "    prediction_df['Predicitons'] = prediction_df[['gloss','predicted_gloss']].apply(lambda row: True if row[0] == row[1] else False,axis=1)\n",
    "    \n",
    "    return prediction_df.drop(columns=['predicted_gloss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51359b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "savepathval = r\"./data/preprocessed/senseval_gloss.pkl\"\n",
    "final_corpus_val = pd.read_pickle(savepathval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11286f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b0d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = build_pred_dataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe8d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred[pred.is_proper_gloss==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373c6f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff83c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[pred_df.is_proper_gloss ==pred_df.Predicitons].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d06c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "41/85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b74683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
