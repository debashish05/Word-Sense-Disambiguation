{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Necessary Packages**"
      ],
      "metadata": {
        "id": "0HOiG2OPLDyl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YVYRsid0E9G5"
      },
      "outputs": [],
      "source": [
        "import os,glob,pathlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm,trange\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus.reader.wordnet import WordNetError\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import re\n",
        "import itertools\n",
        "from collections import Counter\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import pickle\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras.backend as K"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Packages for Github cloning. Cloning the Dataset from Princton Repo**"
      ],
      "metadata": {
        "id": "-fj8C_zTLKvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gitpython\n",
        "import git\n",
        "nltk.download('wordnet')\n",
        "repo = git.Repo.clone_from(\"https://github.com/rubenIzquierdo/wsd_corpora.git\", \"./data/raw/wsd_corpora\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUQzYkGPLM8H",
        "outputId": "1d60639e-26e4-41aa-e2a8-15d15a541d11"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.7/dist-packages (3.1.27)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython) (4.2.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.0)\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting displaying options**"
      ],
      "metadata": {
        "id": "uVkX9K7nLSi4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWSE7YPUFJ7B"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parsing The Semcor 3.0 Dataset**"
      ],
      "metadata": {
        "id": "P-ZaAteaLcul"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-fsyJqgHnmu"
      },
      "outputs": [],
      "source": [
        "def xml_parse(_fpath):\n",
        "\n",
        "    sctree = ET.parse(_fpath)\n",
        "\n",
        "    # Iterates over list of words in files    \n",
        "    dct_list1 = []\n",
        "    for node in sctree.iter('wf'):\n",
        "        attributes = node.attrib\n",
        "        attributes['text'] = node.text\n",
        "        dct_list1.append(attributes)\n",
        "\n",
        "    # Iterates over terms to find senses and corresponding sense references\n",
        "    dct_list2 = []\n",
        "    for term in sctree.iter('term'):\n",
        "        lemma = term.attrib.get('lemma')\n",
        "        wordid = term.find('span/target').attrib.get('id')\n",
        "        pos = ''\n",
        "\n",
        "        wnsn = '0'\n",
        "        senseid=''\n",
        "        if term.findall('externalReferences/externalRef'):\n",
        "            wnsn = term.findall('externalReferences/externalRef')[0].attrib.get('reference')\n",
        "            senseid = term.findall('externalReferences/externalRef')[1].attrib.get('reference')\n",
        "        dct_list2.append({'id':wordid,'lemma':lemma,'wn_sense_num':wnsn,'lexical_key':senseid,'pos':term.attrib['pos']})\n",
        "\n",
        "    word_df = pd.DataFrame(dct_list1)\n",
        "    sense_ref_df = pd.DataFrame(dct_list2)   \n",
        "    \n",
        "    return pd.merge(word_df,sense_ref_df,on='id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jGUIuilHp9N"
      },
      "outputs": [],
      "source": [
        "def gen_file_list(_basepath,ext='*.naf'):\n",
        "    \n",
        "    file_list = []\n",
        "    fla = glob.glob(os.path.join(_basepath,ext))\n",
        "    flb = glob.glob(os.path.join(_basepath,'*',ext))\n",
        "    flc = glob.glob(os.path.join(_basepath,'**',ext))\n",
        "    files = set(fla+flb+flc)\n",
        "    for fileref in files: #search recursively for files\n",
        "        parent_folder_name = pathlib.Path(fileref).parent.name\n",
        "        file_name = pathlib.Path(fileref).name.split('.')[0]\n",
        "        \n",
        "        file_list.append( {'file_path':fileref,\n",
        "                           'parent_folder':parent_folder_name,\n",
        "                           'file_name':file_name})\n",
        "    return pd.DataFrame(file_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuBPcEQCHq45"
      },
      "outputs": [],
      "source": [
        "def parse_corpus(_basepath,filter_validation = False):\n",
        "\n",
        "   # generate dataframe with references to all files\n",
        "    _fpath_df = gen_file_list(_basepath)\n",
        "    \n",
        "    # filter to remove validation files\n",
        "    filtered_file_df = _fpath_df\n",
        "    if filter_validation:\n",
        "         filtered_file_df = _fpath_df[_fpath_df.parent_folder != 'brownv']\n",
        "    \n",
        "    _dflist = []\n",
        "    for i,file_entry in tqdm(filtered_file_df.iterrows(), total=filtered_file_df.shape[0]):\n",
        "        _parsed_file_df = xml_parse(file_entry.file_path)\n",
        "        #print(_parsed_file_df.head())\n",
        "        _parsed_file_df['file'] = file_entry.file_name\n",
        "        _dflist.append(_parsed_file_df)\n",
        "\n",
        "    return pd.concat(_dflist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vol1KLRhHuF8"
      },
      "outputs": [],
      "source": [
        "def build_corpus(_basepath,verbose=True,**kwargs):\n",
        "    if verbose: print('Parsing corpus')\n",
        "    base_corpus = parse_corpus(_basepath,**kwargs)\n",
        "\n",
        "    # Build wordnet ref key using wordnet lemma\n",
        "    if verbose: print('Preprocessing indexes...',end=\"\")\n",
        "    base_corpus['wn_index'] = base_corpus['lemma']+'%'+base_corpus['lexical_key']\n",
        "\n",
        "    base_corpus.loc[base_corpus.lexical_key == '','wn_index'] = ''\n",
        "    base_corpus.drop('lexical_key',axis=1,inplace=True)\n",
        "    if verbose: print('Done!')\n",
        "    print(base_corpus.shape)\n",
        "    return base_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Locznd4LHwOd"
      },
      "outputs": [],
      "source": [
        "def wordnet_get_glosses(_word,_sense_id):\n",
        "    _sense_id = int(_sense_id)\n",
        "    if not _word: \n",
        "        return ''\n",
        "    try:\n",
        "        all_synsets = wn.synsets(_word)\n",
        "        target_gloss = []\n",
        "        other_glosses = []\n",
        "        for syn in all_synsets:\n",
        "            split = syn.name().split('.')\n",
        "            wn_lemma = split[0]\n",
        "            sense_num = int(split[-1])\n",
        "            if sense_num == _sense_id:\n",
        "                target_gloss.append(syn.definition()) \n",
        "            else:\n",
        "                other_glosses.append(syn.definition())                \n",
        "        return target_gloss,other_glosses\n",
        "    except (AttributeError,WordNetError,ValueError) as err:\n",
        "        return 'WN Error',None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYrJNnw0HyPO"
      },
      "outputs": [],
      "source": [
        "def wordnet_gloss_helper(_word,_sense_id):\n",
        "    if not _word or not _sense_id:\n",
        "        return '',''\n",
        "    senseidlist = _sense_id.split(';')\n",
        "    if len(senseidlist) == 1:\n",
        "        return wordnet_get_glosses(_word,int(_sense_id))\n",
        "    elif len(senseidlist) > 1:\n",
        "        list_proper_glosses = []\n",
        "        other_gloss_set = set()\n",
        "        for senseid in senseidlist:\n",
        "            gloss, other_glosses =  wordnet_get_glosses(_word,int(senseid))\n",
        "            if gloss:\n",
        "                list_proper_glosses.append(gloss)\n",
        "                other_gloss_set.update(set(other_glosses))\n",
        "        # if one of the glosses is bogus return only one\n",
        "        if len(list_proper_glosses) == 1:\n",
        "            return list_proper_glosses[0], other_gloss_set\n",
        "        return list_proper_glosses, other_gloss_set\n",
        "    else:\n",
        "        return  'WN Error',[]   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JY__AxCH0qv"
      },
      "outputs": [],
      "source": [
        "def add_wordnet_gloss(_semcordf,verbose=True):\n",
        "\n",
        "    if verbose: print('Adding wordnet glosses')\n",
        "    _semcordf['idx'] = list(range(len(_semcordf))) #adding index for merging\n",
        "    tqdm.pandas(desc=\"Gloss preprocessing\") \n",
        "    _glosses = _semcordf[_semcordf.wn_sense_num != '0'].progress_apply(lambda _row: (*wordnet_gloss_helper(_row['lemma'],_row['wn_sense_num'])\\\n",
        "                                                                        ,_row['idx']),axis=1 )\n",
        "    _df_glosses = pd.DataFrame(_glosses.values.tolist(),columns=['gloss','other_glosses','idx'])\n",
        "    _merged = pd.merge(_semcordf,_df_glosses,on='idx',how='left').fillna('')\n",
        "    # for now take only first gloss\n",
        "    _merged['gloss'] = _merged.gloss.apply(lambda x: x[0] if x else '')\n",
        "    # tag how many other glosses there are\n",
        "    _merged['other_glossesnum'] = _merged.other_glosses.apply(lambda x: len(x))   \n",
        "    if verbose: print('Done!')   \n",
        "    return _merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-F6aLwnH4TY"
      },
      "outputs": [],
      "source": [
        "def gen_sentence_context_pairs(_df):\n",
        "    sentence = _df.text.str.cat(sep = ' ').replace(\" '\",\"'\")\n",
        "    basedct = {'context':sentence,\n",
        "               'file':_df.iloc[0].file}\n",
        "\n",
        "    semcor_sentences = []\n",
        "\n",
        "    # Make sure there are other glosses and that the gloss column is not null\n",
        "    for i,(j,line) in enumerate(_df[(_df.other_glossesnum > 0) & (_df.gloss != 'WN Error') & (_df.gloss != '')].iterrows()): \n",
        "\n",
        "        newbasedct = basedct.copy()\n",
        "        newbasedct['target_word'] = line.text\n",
        "        newbasedct['gloss'] = line.gloss\n",
        "        newbasedct['is_proper_gloss'] = True\n",
        "        semcor_sentences.append(newbasedct)\n",
        "        # Then append all different contexes with False labels\n",
        "        for other_glosses in line.other_glosses:\n",
        "            newbasedct = basedct.copy()\n",
        "            newbasedct['target_word'] = line.text\n",
        "            newbasedct['gloss'] = other_glosses\n",
        "            newbasedct['is_proper_gloss'] = False\n",
        "            semcor_sentences.append(newbasedct)\n",
        "                \n",
        "    return semcor_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lU9nW34yH7pv"
      },
      "outputs": [],
      "source": [
        "def build_joint_dataset(_df):\n",
        "    groupbyobj = _df.groupby(['sent','file'])\n",
        "    full_dict_list = []\n",
        "    for [sentnum,file],gp in tqdm(groupbyobj,total=len(groupbyobj)):\n",
        "        full_dict_list.extend(gen_sentence_context_pairs(gp))\n",
        "    cols = ['file','context','target_word','gloss','is_proper_gloss']\n",
        "    return pd.DataFrame(full_dict_list)[cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dEZnrcNH90u"
      },
      "outputs": [],
      "source": [
        "def build_corpus_dataset(_basepath,verbose=True,byref=False):\n",
        "    \n",
        "    corpus_df = build_corpus(_basepath,verbose=verbose)\n",
        "    corpus_df = add_wordnet_gloss(corpus_df,verbose=verbose)\n",
        "    if verbose: print('Processing adn labeling joint context-gloss pairs...',end=\"\")\n",
        "    final_corpus = build_joint_dataset(corpus_df)\n",
        "    if verbose: print('Done!')    \n",
        "    return final_corpus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEqe3w3KlYxR",
        "outputId": "1918f490-44eb-48b3-a0b4-15697ff71f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving the Final Corpus**"
      ],
      "metadata": {
        "id": "QYX6sWhAMB74"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLHRqrkiIDfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07acacb8-eb7d-45ae-ebd0-6131b0b6e9df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing corpus\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:33<00:00, 10.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing indexes...Done!\n",
            "(676546, 10)\n",
            "Adding wordnet glosses\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gloss preprocessing: 100%|██████████| 226040/226040 [00:24<00:00, 9158.75it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n",
            "Processing adn labeling joint context-gloss pairs..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 37168/37168 [01:57<00:00, 316.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "fpath = \"./notebooks/data/raw/semcor3.0/\"\n",
        "savepath = r\"./notebooks/data/preprocessed/semcor_gloss_BiLSTM.pkl\"\n",
        "final_corpus = build_corpus_dataset(fpath)\n",
        "final_corpus.to_pickle(savepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading the Final Saved Corpus**"
      ],
      "metadata": {
        "id": "CZJzF9uLMKBk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdnDbJ3wr3IC"
      },
      "outputs": [],
      "source": [
        "final_corpus = pd.read_pickle(savepath)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orig_final_corpus = final_corpus\n",
        "orig_final_corpus.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG1Xj2WujxNQ",
        "outputId": "da36ea8d-a28d-45ee-898a-105eb52c425d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1589759, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_corpus[final_corpus.is_proper_gloss==True].shape,final_corpus[final_corpus.is_proper_gloss==False].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YklfSzIBjxQq",
        "outputId": "e9221898-95d1-4ad6-8155-805b2f4a3bb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((178570, 5), (1411189, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Under Sampling the data**"
      ],
      "metadata": {
        "id": "J-pQCPXkMNZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_df, y_df  = final_corpus.iloc[:, :-1] , final_corpus.iloc[:, -1]\n",
        "\n",
        "under = RandomUnderSampler(sampling_strategy=0.4)\n",
        "X_df, y_df = under.fit_resample(X_df, y_df)\n",
        "\n",
        "over = RandomOverSampler(sampling_strategy=.5)\n",
        "X_df,y_df = over.fit_resample(X_df , y_df)\n",
        "\n",
        "X_df['is_proper_gloss'] = y_df\n",
        "final_corpus = X_df"
      ],
      "metadata": {
        "id": "taikZWPkjxT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_corpus[final_corpus.is_proper_gloss==True].shape,final_corpus[final_corpus.is_proper_gloss==False].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exJCv4tIjxW9",
        "outputId": "ae36a73f-0653-436b-af79-632b0f234878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((223212, 5), (446425, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_corpus = final_corpus.sample(frac=1)"
      ],
      "metadata": {
        "id": "nGr7D-UXjxai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train and Test Split**"
      ],
      "metadata": {
        "id": "0B96J_DyMQjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df =  train_test_split(final_corpus, \n",
        "                                        random_state=None, \n",
        "                                        test_size=.1)\n",
        "val_df, test_df =  train_test_split(val_df, \n",
        "                                        random_state=None, \n",
        "                                        test_size=.1)"
      ],
      "metadata": {
        "id": "BHzY015qjxdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape , val_df.shape, test_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRaLy8VCjxgl",
        "outputId": "2936d712-b9f2-4cf1-81e0-ad3d6e7f754f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((602673, 5), (60267, 5), (6697, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loader**"
      ],
      "metadata": {
        "id": "JHZj7qMqMVUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLDataGen(tf.keras.utils.Sequence):\n",
        "    \n",
        "    def __init__(self, df, batch_size = 64, gen_type=  'train', model_file_lang1 = None, model_file_lang2 = None):\n",
        "        self.context =list(map(lambda row:  ''.join(char for char in row[2] if char.isalnum())+' '+row[1]+' '+ ''.join(char for char in row[2] if char.isalnum()),df.to_numpy())) \n",
        "        self.tagged_sense = list(map(lambda row: str(row[3]),df.to_numpy())) \n",
        "        self.label= df['is_proper_gloss'].astype(int).to_numpy()\n",
        "        self.gen_type = gen_type\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        self.max_seq_len=50\n",
        "        self.x0_vocab_size =0\n",
        "        self.x1_vocab_size =0\n",
        "        self.x0_tokenizer = None\n",
        "        self.x1_tokenizer = None\n",
        "        self._x0=[]\n",
        "        self._x1=[]\n",
        "        self._y=[]\n",
        "        self._initialize_variables()\n",
        "        self.model_file_lang1 = model_file_lang1\n",
        "        self.model_file_lang2 = model_file_lang2\n",
        "\n",
        "\n",
        "    def _initialize_variables(self):         \n",
        "        \n",
        "        if self.gen_type == 'train':\n",
        "            tokenized = [line.split() for line in self.context]\n",
        "            flattendata = list(itertools.chain.from_iterable(tokenized))\n",
        "            word_freq = dict(Counter(flattendata))\n",
        "            tot_cnt  = len(word_freq)\n",
        "            self.x0_tokenizer = Tokenizer(num_words = tot_cnt+1 ,oov_token= '<OOV>')\n",
        "            # saving\n",
        "            with open('tokenizer_lang1.pickle', 'wb') as handle:\n",
        "                pickle.dump(self.x0_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        else:\n",
        "\n",
        "            # loading\n",
        "            with open('tokenizer_lang1.pickle', 'rb') as handle:\n",
        "                self.x0_tokenizer = pickle.load(handle)\n",
        "                \n",
        "            if self.x0_tokenizer  ==None:\n",
        "                print('Vocabulary is not set. Try training mode')\n",
        "                \n",
        "        self.x0_tokenizer.fit_on_texts(list(self.context))\n",
        "        self.x0_vocab_size = self.x0_tokenizer.num_words + 1\n",
        "        \n",
        "        # Convert text sequences to integer sequences \n",
        "        x0_train_seq = self.x0_tokenizer.texts_to_sequences(self.context) \n",
        "\n",
        "        # Pad zero upto maximum length\n",
        "        x0_train = pad_sequences(x0_train_seq,  maxlen=self.max_seq_len, padding='post')\n",
        "        \n",
        "        if self.gen_type == 'train':\n",
        "            tokenized = [line.strip().split() for line in self.tagged_sense]\n",
        "            flattendata = list(itertools.chain.from_iterable(tokenized))\n",
        "            word_freq = dict(Counter(flattendata))\n",
        "            tot_cnt  = len(word_freq)\n",
        "            self.x1_tokenizer = Tokenizer(num_words = tot_cnt ,oov_token= '<OOV>')\n",
        "            # saving\n",
        "            with open('tokenizer_lang2.pickle', 'wb') as handle:\n",
        "                pickle.dump(self.x1_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        else:\n",
        "\n",
        "            # loading\n",
        "            with open('tokenizer_lang2.pickle', 'rb') as handle:\n",
        "                self.x1_tokenizer = pickle.load(handle)\n",
        "                \n",
        "            if self.x1_tokenizer  ==None:\n",
        "                print('Vocabulary is not set. Try training mode')\n",
        "\n",
        "\n",
        " \n",
        "        self.x1_tokenizer.fit_on_texts(list(self.tagged_sense))\n",
        "        self.x1_vocab_size = self.x1_tokenizer.num_words + 1\n",
        "        \n",
        "        # Convert text sequences to integer sequences \n",
        "        x1_train_seq = self.x1_tokenizer.texts_to_sequences(self.tagged_sense)\n",
        "\n",
        "        # Pad zero upto maximum length\n",
        "        x1_train = pad_sequences(x1_train_seq,  maxlen=self.max_seq_len, padding='post')\n",
        "\n",
        "        self._x0 = x0_train\n",
        "        self._x1 = x1_train\n",
        "        self._y = self.label\n",
        "    \n",
        "    \n",
        "        \n",
        "    def get_data_batch(self,i):           \n",
        "       \n",
        "        x0 = self._x0[i * self.batch_size:(i + 1) * self.batch_size]\n",
        "        x1 = self._x1[i * self.batch_size:(i + 1) * self.batch_size]       \n",
        "        y = self._y[i * self.batch_size:(i + 1) * self.batch_size].reshape(-1,1)\n",
        "        return [np.array(x0), np.array(x1)],np.array(y)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        X, y = self.get_data_batch(index)\n",
        "        return X,y\n",
        "     \n",
        "    def __len__(self):\n",
        "        return len(self._x0) // self.batch_size"
      ],
      "metadata": {
        "id": "8iQD-mAujxj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation and Train Data**"
      ],
      "metadata": {
        "id": "K2BqfPrcMa8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_gen = MLDataGen(train_df,batch_size=256)\n",
        "val_gen = MLDataGen(val_df, gen_type='val',batch_size=64)"
      ],
      "metadata": {
        "id": "vKkIsGOYjxqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Glove Embedding**"
      ],
      "metadata": {
        "id": "vKxUQRMUMgcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTEAVYqRMlHb",
        "outputId": "493184d8-fce9-423c-8168-fb804aeac69f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-01 15:17:14--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-05-01 15:17:15--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-05-01 15:17:15--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.11MB/s    in 2m 41s  \n",
            "\n",
            "2022-05-01 15:19:57 (5.10 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embeddings of the data using Glove**"
      ],
      "metadata": {
        "id": "Eiqw4-OIMqnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_glove_file = './glove.6B.100d.txt'\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drVgPTCRjxuP",
        "outputId": "7302a8ad-2a1c-46a2-9bd7-cf6f727e64b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index_0 = data_gen.x0_tokenizer.word_index\n",
        "word_index_1 = data_gen.x1_tokenizer.word_index\n",
        "\n",
        "num_tokens_0 = (data_gen.x0_vocab_size) + 2\n",
        "num_tokens_1 = (data_gen.x1_vocab_size) + 2"
      ],
      "metadata": {
        "id": "JlNhGgtUjxxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding Matrix**"
      ],
      "metadata": {
        "id": "XGsaJGNhMxe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 50\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix_0 = np.zeros((num_tokens_0, embedding_dim))\n",
        "for word, i in word_index_0.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix_0[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtYbSjVMjx09",
        "outputId": "62dda16c-6169-4055-a11f-35c20ef8e33d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 33186 words (6794 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix_1 = np.zeros((num_tokens_1, embedding_dim))\n",
        "for word, i in word_index_1.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix_1[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GLormkZjx7N",
        "outputId": "9515fb39-5d13-4d83-caf2-d40e92f1cf19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 50245 words (7989 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nueral Network Architecture**"
      ],
      "metadata": {
        "id": "ZuSFa1WiM2hF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 64\n",
        "embedding_dim = 50\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(data_gen.max_seq_len, ))\n",
        "\n",
        "# Embedding layer\n",
        "enc_emb = Embedding(num_tokens_0 , \n",
        "                    embedding_dim, \n",
        "                    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix_0),\n",
        "                    trainable=False,\n",
        "                    mask_zero=True)(encoder_inputs)\n",
        "\n",
        "# Encoder LSTM 1\n",
        "encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.1,recurrent_dropout=0.1)\n",
        "(encoder_output1, state_h, state_c) = encoder_lstm1(enc_emb)\n",
        "\n",
        "# Set up the decoder, using encoder_states as the initial state\n",
        "decoder_inputs = Input(shape=(None, ))\n",
        "\n",
        "# Embedding layer\n",
        "dec_emb_layer = Embedding(num_tokens_1 , \n",
        "                          embedding_dim, \n",
        "                          embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix_1),\n",
        "                          trainable=False,\n",
        "                          mask_zero=True)\n",
        "\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# Decoder LSTM\n",
        "decoder_lstm = LSTM(latent_dim, dropout=0.1,recurrent_dropout=0.1)\n",
        "decoder_outputs = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "\n",
        "# Dense layer\n",
        "decoder_dense1 = Dense(64, activation='relu')#Dense(2, activation='softmax')\n",
        "decoder_outputs = decoder_dense1(decoder_outputs)\n",
        "decoder_dense2 = Dense(1, activation='sigmoid')\n",
        "decoder_outputs = decoder_dense2(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCqxuD-ujx-u",
        "outputId": "728228c0-3f90-4ff9-e401-c09ca37f1490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 50)]         0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 50, 50)       2824150     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 50)     1210400     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 50, 64),     29440       ['embedding[0][0]']              \n",
            "                                 (None, 64),                                                      \n",
            "                                 (None, 64)]                                                      \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  (None, 64)           29440       ['embedding_1[0][0]',            \n",
            "                                                                  'lstm[0][1]',                   \n",
            "                                                                  'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 64)           4160        ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            65          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,097,655\n",
            "Trainable params: 63,105\n",
            "Non-trainable params: 4,034,550\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**F1 Score Calculation**"
      ],
      "metadata": {
        "id": "aYG8-VraNOAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_score(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "metadata": {
        "id": "63aVxHuYjyCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compiling and Saving checkpoints. Also added Early Stopping**"
      ],
      "metadata": {
        "id": "SxIaDn8mNTQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adam = Adam(learning_rate=0.01)\n",
        "model.compile( loss=tf.keras.losses.BinaryCrossentropy(), \n",
        "                optimizer='rmsprop', \n",
        "                metrics=[get_score,'accuracy'])\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
        "\n",
        "from keras.callbacks import Callback,ModelCheckpoint\n",
        "\n",
        "checkpoint_path = \"./data/preprocessed/checkpoints/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "   checkpoint_path, verbose=1, save_weights_only=True,\n",
        "   # Save weights, every epoch.\n",
        "   save_freq='epoch')"
      ],
      "metadata": {
        "id": "jhC5YpygjyGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fitting the Model**"
      ],
      "metadata": {
        "id": "ZpnPuLJ7NfNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    data_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=10,\n",
        "    callbacks=[es,cp_callback],\n",
        "    verbose=1\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILKef8zQjyJN",
        "outputId": "e1d82d66-7010-414c-8961-4da214d1774f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2354/2354 [==============================] - ETA: 0s - loss: 0.4606 - get_score: 0.6136 - accuracy: 0.7711\n",
            "Epoch 1: saving model to ./data/preprocessed/checkpoints/cp-0001.ckpt\n",
            "2354/2354 [==============================] - 1908s 806ms/step - loss: 0.4606 - get_score: 0.6136 - accuracy: 0.7711 - val_loss: 0.6252 - val_get_score: 0.3769 - val_accuracy: 0.6887\n",
            "Epoch 2/10\n",
            "2354/2354 [==============================] - ETA: 0s - loss: 0.3866 - get_score: 0.7156 - accuracy: 0.8188\n",
            "Epoch 2: saving model to ./data/preprocessed/checkpoints/cp-0002.ckpt\n",
            "2354/2354 [==============================] - 1870s 794ms/step - loss: 0.3866 - get_score: 0.7156 - accuracy: 0.8188 - val_loss: 0.6862 - val_get_score: 0.3766 - val_accuracy: 0.6877\n",
            "Epoch 3/10\n",
            "2354/2354 [==============================] - ETA: 0s - loss: 0.3588 - get_score: 0.7454 - accuracy: 0.8358\n",
            "Epoch 3: saving model to ./data/preprocessed/checkpoints/cp-0003.ckpt\n",
            "2354/2354 [==============================] - 1922s 817ms/step - loss: 0.3588 - get_score: 0.7454 - accuracy: 0.8358 - val_loss: 0.6867 - val_get_score: 0.3723 - val_accuracy: 0.6884\n",
            "Epoch 3: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_target_word_index = data_gen.x0_tokenizer.index_word\n",
        "reverse_source_word_index = data_gen.x0_tokenizer.index_word\n",
        "target_word_index = data_gen.x1_tokenizer.word_index"
      ],
      "metadata": {
        "id": "Sp8bi3a8jyQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference Models\n",
        "\n",
        "# Encode the input sequence to get the feature vector\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_output1,\n",
        "                      state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim, ))\n",
        "decoder_state_input_c = Input(shape=(latent_dim, ))\n",
        "decoder_hidden_state_input = Input(shape=(data_gen.max_seq_len, latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2 = decoder_lstm(dec_emb2,\n",
        "        initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense1(decoder_outputs2)\n",
        "decoder_outputs2 = decoder_dense2(decoder_outputs2)\n",
        "\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,\n",
        "                      decoder_state_input_h, decoder_state_input_c],\n",
        "                      [decoder_outputs2])"
      ],
      "metadata": {
        "id": "QPMXdlhEjyTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wn.ADJ,\n",
        "                \"N\": wn.NOUN,\n",
        "                \"V\": wn.VERB,\n",
        "                \"R\": wn.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wn.NOUN)\n"
      ],
      "metadata": {
        "id": "8pAEiLTZjyXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sense Prediction**"
      ],
      "metadata": {
        "id": "jJE472dlNja8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NGpwOwYr3EY"
      },
      "outputs": [],
      "source": [
        "def predictSense(sentence, target):\n",
        "    targets=[]\n",
        "    if '_' in target:\n",
        "        targets = target.split('_')\n",
        "    else:\n",
        "        targets=[target]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for t in targets:\n",
        "        lemmas.append(lemmatizer.lemmatize(t, get_wordnet_pos(t)))\n",
        "    lemma = '_'.join(lemmas)\n",
        "    all_synsets = wn.synsets(lemma)\n",
        "    glosses = []\n",
        "    for syn in all_synsets:\n",
        "        glosses.append(syn.definition())\n",
        "    \n",
        "    if len(glosses)==0:\n",
        "        return ''    \n",
        "    \n",
        "    x0_test_seq = data_gen.x0_tokenizer.texts_to_sequences([sentence+' '+target]*len(glosses))\n",
        "    x0_test = pad_sequences(x0_test_seq,  maxlen=data_gen.max_seq_len, padding='post')\n",
        "\n",
        "    x1_test_seq = data_gen.x1_tokenizer.texts_to_sequences(glosses)\n",
        "    x1_test = pad_sequences(x1_test_seq,  maxlen=data_gen.max_seq_len, padding='post')\n",
        "    \n",
        "    #return x0_test , x1_test    \n",
        "    try:\n",
        "        (e_out, e_h, e_c) = encoder_model.predict(x0_test)\n",
        "\n",
        "        ypred = decoder_model.predict([x1_test] + [e_out, e_h, e_c])\n",
        "    except:\n",
        "        print(lemma,targets)\n",
        "    return glosses[np.argmax(ypred)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Set Data preprations**"
      ],
      "metadata": {
        "id": "vy5Bqgk8Nqpw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdqUwzw3r3BW"
      },
      "outputs": [],
      "source": [
        "def build_pred_dataset(_df):\n",
        "    context_target = _df[['context','target_word']].drop_duplicates()\n",
        "    print(context_target.shape,_df.shape)\n",
        "    basedct = {}\n",
        "    predictions = []\n",
        "    context_target['predicted_gloss'] = context_target[['context','target_word']].apply(lambda row: predictSense(row[0],row[1]),axis=1)\n",
        "    prediction_df = pd.merge(_df,context_target,on=['context','target_word'],how='right').fillna('')\n",
        "    prediction_df['Predicitons'] = prediction_df[['gloss','predicted_gloss']].apply(lambda row: True if row[0] == row[1] else False,axis=1)\n",
        "    \n",
        "    return prediction_df.drop(columns=['predicted_gloss'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwldz2ti8Vcq",
        "outputId": "dcc0ccc4-a679-4eaa-c571-4448e6595367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results**"
      ],
      "metadata": {
        "id": "mJlpfE7sN-6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = build_pred_dataset(test_df[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFr1FfhzoB7K",
        "outputId": "407ff3a6-6dcd-4cd4-a8e5-a8f67fb4f896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(499, 2) (500, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_df = pred[pred.is_proper_gloss==True]"
      ],
      "metadata": {
        "id": "TaTk1axNoB91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_df.shape , pred_df[pred_df.is_proper_gloss ==pred_df.Predicitons].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwHypLCHoCAt",
        "outputId": "1d51783c-250b-4799-af57-c1197ab20b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((170, 6), (98, 6))"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}