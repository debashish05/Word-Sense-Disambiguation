{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Dependencies**"
      ],
      "metadata": {
        "id": "J40TwAfwPqrc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KQyl7lppInq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78c6408a-68bd-45b1-946d-3c34bbeaf3a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-transformers\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[K     |████████████████████████████████| 176 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 49.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.11.0+cu113)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 35.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (4.64.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.22.4-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 51.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers) (4.2.0)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting botocore<1.26.0,>=1.25.4\n",
            "  Downloading botocore-1.25.4-py3-none-any.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 43.1 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.26.0,>=1.25.4->boto3->pytorch-transformers) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 35.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.26.0,>=1.25.4->boto3->pytorch-transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 47.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, sacremoses, boto3, pytorch-transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.22.4 botocore-1.25.4 jmespath-1.0.0 pytorch-transformers-1.2.0 s3transfer-0.5.2 sacremoses-0.0.49 sentencepiece-0.1.96 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "!pip install  pytorch-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YVYRsid0E9G5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d3f0018-215d-4ffa-ec6c-dc180f2e3702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import os,glob,pathlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "import os\n",
        "import pytorch_transformers\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm,trange\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus.reader.wordnet import WordNetError\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from pytorch_transformers import BertTokenizer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
        "import torch\n",
        "from torch.autograd import Function\n",
        "from torch import nn\n",
        "from torch import arange, zeros_like\n",
        "from pytorch_transformers import BertModel, BertConfig\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Packages for Github cloning. Cloning the Dataset from Princton Repo**"
      ],
      "metadata": {
        "id": "cRFDvSl3P36v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gitpython\n",
        "import git\n",
        "repo = git.Repo.clone_from(\"https://github.com/rubenIzquierdo/wsd_corpora.git\", \"./data/raw/wsd_corpora\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Sw5Bi-gP45r",
        "outputId": "cf499cb5-4c69-44d5-e304-0246197a4e5a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.9 gitpython-3.1.27 smmap-5.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting displaying options**"
      ],
      "metadata": {
        "id": "yoJE7DQiQIQP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uWSE7YPUFJ7B"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parsing The Semcor 3.0 Dataset**"
      ],
      "metadata": {
        "id": "hyi4EvbQQmi0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "C-fsyJqgHnmu"
      },
      "outputs": [],
      "source": [
        "def xml_parse(_fpath):\n",
        "\n",
        "    \n",
        "    sctree = ET.parse(_fpath)\n",
        "\n",
        "    # Iterates over list of words in files    \n",
        "    dct_list1 = []\n",
        "    for node in sctree.iter('wf'):\n",
        "        attributes = node.attrib\n",
        "        attributes['text'] = node.text\n",
        "        dct_list1.append(attributes)\n",
        "\n",
        "    # Iterates over terms to find senses and corresponding sense references\n",
        "    dct_list2 = []\n",
        "    for term in sctree.iter('term'):\n",
        "        lemma = term.attrib.get('lemma')\n",
        "        wordid = term.find('span/target').attrib.get('id')\n",
        "        pos = ''\n",
        "\n",
        "        wnsn = '0'\n",
        "        senseid=''\n",
        "        if term.findall('externalReferences/externalRef'):\n",
        "            wnsn = term.findall('externalReferences/externalRef')[0].attrib.get('reference')\n",
        "            senseid = term.findall('externalReferences/externalRef')[1].attrib.get('reference')\n",
        "        dct_list2.append({'id':wordid,'lemma':lemma,'wn_sense_num':wnsn,'lexical_key':senseid,'pos':term.attrib['pos']})\n",
        "\n",
        "    word_df = pd.DataFrame(dct_list1)\n",
        "    sense_ref_df = pd.DataFrame(dct_list2)   \n",
        "    \n",
        "    return pd.merge(word_df,sense_ref_df,on='id')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1jGUIuilHp9N"
      },
      "outputs": [],
      "source": [
        "def gen_file_list(_basepath,ext='*.naf'):    \n",
        "    file_list = []\n",
        "    fla = glob.glob(os.path.join(_basepath,ext))\n",
        "    flb = glob.glob(os.path.join(_basepath,'*',ext))\n",
        "    flc = glob.glob(os.path.join(_basepath,'**',ext))\n",
        "    files = set(fla+flb+flc)\n",
        "    for fileref in files: #search recursively for files\n",
        "        parent_folder_name = pathlib.Path(fileref).parent.name\n",
        "        file_name = pathlib.Path(fileref).name.split('.')[0]\n",
        "        \n",
        "        file_list.append( {'file_path':fileref,\n",
        "                           'parent_folder':parent_folder_name,\n",
        "                           'file_name':file_name})\n",
        "\n",
        "    return pd.DataFrame(file_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OuBPcEQCHq45"
      },
      "outputs": [],
      "source": [
        "def parse_corpus(_basepath,filter_validation = False):\n",
        "\n",
        "   # generate dataframe with references to all files\n",
        "    _fpath_df = gen_file_list(_basepath)\n",
        "    \n",
        "    # filter to remove validation files\n",
        "    filtered_file_df = _fpath_df\n",
        "    if filter_validation:\n",
        "         filtered_file_df = _fpath_df[_fpath_df.parent_folder != 'brownv']\n",
        "    \n",
        "    _dflist = []\n",
        "    for i,file_entry in tqdm(filtered_file_df.iterrows(), total=filtered_file_df.shape[0]):\n",
        "        _parsed_file_df = xml_parse(file_entry.file_path)\n",
        "        _parsed_file_df['file'] = file_entry.file_name\n",
        "        _dflist.append(_parsed_file_df)\n",
        "\n",
        "    return pd.concat(_dflist)[:5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vol1KLRhHuF8"
      },
      "outputs": [],
      "source": [
        "def build_corpus(_basepath,verbose=True,**kwargs):\n",
        "\n",
        "    if verbose: print('Parsing corpus')\n",
        "    base_corpus = parse_corpus(_basepath,**kwargs)\n",
        "\n",
        "    # Build wordnet ref key using wordnet lemma\n",
        "    if verbose: print('Preprocessing indexes...',end=\"\")\n",
        "    base_corpus['wn_index'] = base_corpus['lemma']+'%'+base_corpus['lexical_key']\n",
        "\n",
        "    base_corpus.loc[base_corpus.lexical_key == '','wn_index'] = ''\n",
        "    base_corpus.drop('lexical_key',axis=1,inplace=True)\n",
        "    if verbose: print('Done!')\n",
        "    return base_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Locznd4LHwOd"
      },
      "outputs": [],
      "source": [
        "def wordnet_get_glosses(_word,_sense_id):\n",
        "\n",
        "    _sense_id = int(_sense_id)\n",
        "    if not _word: # if ref is empty\n",
        "        return ''\n",
        "    try:\n",
        "        all_synsets = wn.synsets(_word)\n",
        "        target_gloss = []\n",
        "        other_glosses = []\n",
        "        for syn in all_synsets:\n",
        "            split = syn.name().split('.')\n",
        "            wn_lemma = split[0]\n",
        "            sense_num = int(split[-1])\n",
        "            if sense_num == _sense_id:\n",
        "                target_gloss.append(syn.definition()) \n",
        "            else:\n",
        "                other_glosses.append(syn.definition())                \n",
        "        return target_gloss,other_glosses[:2]\n",
        "    except (AttributeError,WordNetError,ValueError) as err:\n",
        "        return 'WN Error',None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NYrJNnw0HyPO"
      },
      "outputs": [],
      "source": [
        "def wordnet_gloss_helper(_word,_sense_id):\n",
        "\n",
        "  if not _word or not _sense_id:\n",
        "      return '',''\n",
        "  senseidlist = _sense_id.split(';')\n",
        "  if len(senseidlist) == 1:\n",
        "      return wordnet_get_glosses(_word,int(_sense_id))\n",
        "  elif len(senseidlist) > 1:\n",
        "      list_proper_glosses = []\n",
        "      other_gloss_set = set()\n",
        "      for senseid in senseidlist:\n",
        "          gloss, other_glosses =  wordnet_get_glosses(_word,int(senseid))\n",
        "          if gloss:\n",
        "              list_proper_glosses.append(gloss)\n",
        "              other_gloss_set.update(set(other_glosses))\n",
        "      # if one of the glosses is bogus return only one\n",
        "      if len(list_proper_glosses) == 1:\n",
        "          return list_proper_glosses[0], other_gloss_set\n",
        "      return list_proper_glosses, other_gloss_set\n",
        "  else:\n",
        "      return  'WN Error',[]   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5JY__AxCH0qv"
      },
      "outputs": [],
      "source": [
        "def add_wordnet_gloss(_semcordf,verbose=True):\n",
        "\n",
        "    #if verbose: print('Adding wordnet glosses')\n",
        "    _semcordf['idx'] = list(range(len(_semcordf))) #adding index for merging\n",
        "    tqdm.pandas(desc=\"Gloss preprocessing\") \n",
        "    _glosses = _semcordf[_semcordf.wn_sense_num != '0'].progress_apply(lambda _row: (*wordnet_gloss_helper(_row['lemma'],_row['wn_sense_num'])\\\n",
        "                                                                        ,_row['idx']),axis=1 )\n",
        "    _df_glosses = pd.DataFrame(_glosses.values.tolist(),columns=['gloss','other_glosses','idx'])\n",
        "    _merged = pd.merge(_semcordf,_df_glosses,on='idx',how='left').fillna('')\n",
        "    # for now take only first gloss\n",
        "    _merged['gloss'] = _merged.gloss.apply(lambda x: x[0] if x else '')\n",
        "    # tag how many other glosses there are\n",
        "    _merged['other_glossesnum'] = _merged.other_glosses.apply(lambda x: len(x))   \n",
        "    if verbose: print('Done!')\n",
        "    return _merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "D-F6aLwnH4TY"
      },
      "outputs": [],
      "source": [
        "def gen_sentence_context_pairs(_df):\n",
        "\n",
        "    concatenated_sentence = _df.text.str.cat(sep = ' ').replace(\" '\",\"'\")\n",
        "    basedct = {'context':concatenated_sentence,\n",
        "               'file':_df.iloc[0].file}\n",
        "\n",
        "    semcor_sentences = []\n",
        "\n",
        "    # Make sure there are other glosses and that the gloss column is not null\n",
        "    for i,line in _df[(_df.other_glossesnum > 0) & (_df.gloss != 'WN Error') & (_df.gloss != '')].iterrows(): \n",
        "\n",
        "        # First append the proper context to dct with label True\n",
        "        newbasedct = basedct.copy()\n",
        "        newbasedct['target_word'] = line.text\n",
        "        newbasedct['gloss'] = line.gloss\n",
        "        newbasedct['is_proper_gloss'] = True\n",
        "        semcor_sentences.append(newbasedct)\n",
        "        # Then append all different contexes with False labels\n",
        "        for other_glosses in line.other_glosses:\n",
        "            newbasedct = basedct.copy()\n",
        "            newbasedct['target_word'] = line.text\n",
        "            newbasedct['gloss'] = other_glosses\n",
        "            newbasedct['is_proper_gloss'] = False\n",
        "            semcor_sentences.append(newbasedct)\n",
        "                \n",
        "    return semcor_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lU9nW34yH7pv"
      },
      "outputs": [],
      "source": [
        "def build_joint_dataset(_df):\n",
        "\n",
        "    groupbyobj = _df.groupby(['sent','file'])\n",
        "    full_dict_list = []\n",
        "    for [sentnum,file],gp in tqdm(groupbyobj,total=len(groupbyobj)):\n",
        "        full_dict_list.extend(gen_sentence_context_pairs(gp))\n",
        "    cols = ['file','context','target_word','gloss','is_proper_gloss']\n",
        "    return pd.DataFrame(full_dict_list)[cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8dEZnrcNH90u"
      },
      "outputs": [],
      "source": [
        "def build_joint_corpus(_basepath,verbose=True,byref=False):\n",
        "\n",
        "    semcor_corpus_df = build_corpus(_basepath,verbose=verbose)\n",
        "    semcor_corpus_df = add_wordnet_gloss(semcor_corpus_df,verbose=verbose)\n",
        "    if verbose: print('Processing adn labeling joint cintext-gloss pairs...',end=\"\")\n",
        "    final_corpus = build_joint_dataset(semcor_corpus_df)\n",
        "    if verbose: print('Done!')\n",
        "    final_corpus['gloss'] = final_corpus.gloss.apply(lambda x: x[0] if type(x) == list else x)\n",
        "    return final_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving the Final Corpus**"
      ],
      "metadata": {
        "id": "S10b4jQPQ3vE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FLHRqrkiIDfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29e84e76-7bb5-4a48-e0c1-d44abfb22409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing corpus\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:38<00:00,  9.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing indexes...Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gloss preprocessing: 100%|██████████| 2499/2499 [00:06<00:00, 382.38it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n",
            "Processing adn labeling joint cintext-gloss pairs..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:00<00:00, 418.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "fpath = \"./data/raw/wsd_corpora/semcor3.0/\"\n",
        "#os.mkdir(\"./data/raw/wsd_corpora/preprocessed\")\n",
        "savepath = r\"./data/raw/wsd_corpora/preprocessed/semcor_gloss_BERT.pkl\"\n",
        "final_corpus = build_joint_corpus(fpath)\n",
        "final_corpus.to_pickle(savepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HyperParameters**"
      ],
      "metadata": {
        "id": "XATC5lS-SSVD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "BWM3shCuIINh"
      },
      "outputs": [],
      "source": [
        "data_path=savepath\n",
        "default_save_path='..\\data'\n",
        "weak_supervision=True\n",
        "preprocess_inputs=True\n",
        "token_layer='sent-cls-ws'\n",
        "batch_size=128\n",
        "val_check_interval=0.05\n",
        "model_type='bert-base-uncased'\n",
        "lr=2e-5\n",
        "weight_decay=0.01\n",
        "epochs=4\n",
        "input_len=128"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Data**"
      ],
      "metadata": {
        "id": "rEV1SNgPSVyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(model_type)\n",
        "train_dataset = pd.read_pickle(data_path)"
      ],
      "metadata": {
        "id": "sDIuSvgDSo1p"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT SETENCE Format**"
      ],
      "metadata": {
        "id": "Z7cHrBodX4Nb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "R4D40SJ2IOb8"
      },
      "outputs": [],
      "source": [
        "def format_sentences_BERT(_row):\n",
        "    return '[CLS] '+_row.loc['context']+' [SEP] '+_row.loc['gloss']+' [SEP]'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "OpAiBFSCIQYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5abd4cac-e601-46b0-a492-445dda20b559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 2929853.36B/s]\n"
          ]
        }
      ],
      "source": [
        "def tokenize_and_index(_df,output_len=128,tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'),\n",
        "                       display_progress = True,formatting_method=format_sentences_BERT):\n",
        "   \n",
        "    tqdm.pandas(desc=\"Sentence preprocessing\")    \n",
        "    _df.loc[:,'preproc_sent'] = _df.progress_apply(formatting_method,axis=1)\n",
        "    tqdm.pandas(desc=\"Sentence Tokenization\")\n",
        "    _df.loc[:,'tokenized_sent'] = _df.preproc_sent.progress_apply(tokenizer.tokenize)\n",
        "    tqdm.pandas(desc=\"Tokenizing target words\")\n",
        "    _df.loc[:,'tokenized_target_word'] = _df.target_word.progress_apply(lambda row: tokenizer.tokenize(row)[0])\n",
        "    tqdm.pandas(desc=\"Converting tokens to embeddings\")\n",
        "    _df.loc[:,'input_ids'] = _df.tokenized_sent.progress_apply(tokenizer.convert_tokens_to_ids)\n",
        "    \n",
        "    padded_input_ids = pad_sequences(_df['input_ids'], \n",
        "                                    maxlen=output_len, dtype=\"long\",padding = \"post\", truncating = \"post\")\n",
        "    _df.loc[:,'input_ids'] = np.split(padded_input_ids, _df.shape[0], axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "C22XNpqhIxlW"
      },
      "outputs": [],
      "source": [
        "def gen_sentence_indexes(_df,output_len=128):\n",
        "    \n",
        "    def get_index_of_sep(_row):\n",
        "        _index_sep_tokens = [i for i,word  in enumerate(_row['tokenized_sent']) \\\n",
        "                           if word == '[SEP]']\n",
        "        _sentence_indexes = np.array([0]*(_index_sep_tokens[0]+1)\\\n",
        "                                     +[1]*(_index_sep_tokens[1]-_index_sep_tokens[0]))\n",
        "        return _sentence_indexes\n",
        "    \n",
        "    tqdm.pandas(desc=\"Indexing sentences\") \n",
        "    _df.loc[:,'sent_indexes'] = _df.progress_apply(get_index_of_sep,axis=1)\n",
        "    padded_sent_idx = pad_sequences(_df['sent_indexes'],\n",
        "                                               maxlen=output_len, dtype=\"long\",\n",
        "                                               padding = \"post\", truncating = \"post\",value=1)\n",
        "    _df.loc[:,'sent_indexes'] = np.split(padded_sent_idx, _df.shape[0], axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LUNUMJVZIzfH"
      },
      "outputs": [],
      "source": [
        "def find_index_of_target_token(_df):\n",
        "    \"\"\"\n",
        "    looks for index of target token in the corresponding tokenized sentence\n",
        "    \n",
        "    \"\"\"\n",
        "    find_token = lambda  _row: [i for i,word  in \\\n",
        "                         enumerate(_row['tokenized_sent']) \\\n",
        "                         if word == _row['tokenized_target_word'].lower()]\n",
        "    tqdm.pandas(desc=\"Finding target token in sentence\") \n",
        "    _df.loc[:,'target_token_idx'] = _df.progress_apply(find_token,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_oauTePFI1pk"
      },
      "outputs": [],
      "source": [
        "def preprocess_model_inputs(_df,sample_size=None, filter_bad_rows=True,\n",
        "                            output_len=128,\n",
        "                            tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'),**kwargs):\n",
        "\n",
        "    \n",
        "    _smpldf = _df\n",
        "    if sample_size:\n",
        "        _smpldf = _df.sample(sample_size)\n",
        "    \n",
        "    tokenize_and_index(_smpldf,output_len=output_len,\n",
        "                       tokenizer=tokenizer)\n",
        "    gen_sentence_indexes(_smpldf,output_len=output_len)\n",
        "    find_index_of_target_token(_smpldf)\n",
        "\n",
        "        \n",
        "    if filter_bad_rows: # rows where the target word index is not found due to cutoff or exceeds tensor size \n",
        "        _smpldf = _smpldf[_smpldf.target_token_idx.apply(lambda x: len(x) !=  0)]\n",
        "        _smpldf = _smpldf[_smpldf.target_token_idx.apply(lambda x: x[0] <  output_len)]\n",
        "\n",
        "    \n",
        "    return _smpldf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gfK-FxZhQN4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a560e970-7090-45f1-f5a6-1fe53d8702f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sentence preprocessing: 100%|██████████| 5654/5654 [00:00<00:00, 30073.14it/s]\n",
            "Sentence Tokenization: 100%|██████████| 5654/5654 [00:04<00:00, 1332.91it/s]\n",
            "Tokenizing target words: 100%|██████████| 5654/5654 [00:00<00:00, 11170.81it/s]\n",
            "Converting tokens to embeddings: 100%|██████████| 5654/5654 [00:00<00:00, 30129.61it/s]\n",
            "Indexing sentences: 100%|██████████| 5654/5654 [00:00<00:00, 46172.29it/s]\n",
            "Finding target token in sentence: 100%|██████████| 5654/5654 [00:00<00:00, 6944.65it/s]\n"
          ]
        }
      ],
      "source": [
        "df_train = preprocess_model_inputs(train_dataset,tokenizer=tokenizer,output_len=input_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATASET**"
      ],
      "metadata": {
        "id": "UIXWPeaLSvzP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "DiDNW-5Mrh5F"
      },
      "outputs": [],
      "source": [
        "class CorpusDataset(Dataset):\n",
        "    \"\"\"\n",
        "    pytorch dataset handling class    \n",
        "    \"\"\"\n",
        "    def __init__(self, data):\n",
        "        self.corpus_dataframe = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.corpus_dataframe.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.corpus_dataframe.iloc[idx]\n",
        "        return (torch.tensor(row['input_ids'][0]),  # Input token encodings\n",
        "                torch.tensor(row['sent_indexes'][0], dtype=torch.int64), # Sentence encoding\n",
        "                torch.tensor(row['target_token_idx'][0], dtype=torch.int64), # Target token indexes\n",
        "                torch.tensor(row['is_proper_gloss'],dtype=torch.int64)) # Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "BGIjZzKHr3Pu"
      },
      "outputs": [],
      "source": [
        "class TokenClsFunction(Function):\n",
        "  \n",
        "    @staticmethod\n",
        "    def forward(ctx, input, target_token_tensor):\n",
        "        ctx.save_for_backward(input,target_token_tensor)\n",
        "        target_token_tensor.requires_grad = False\n",
        "        flattened_target_tensor = target_token_tensor.flatten()\n",
        "        return input[arange(flattened_target_tensor.shape[0]),flattened_target_tensor,:]\n",
        "        \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input1,target_token_tensor = ctx.saved_tensors\n",
        "        grad = zeros_like(input1)\n",
        "        flattened_target_tensor = target_token_tensor.flatten()\n",
        "        # gradient only flows to specific indexes of target tensor\n",
        "        grad[arange(flattened_target_tensor.shape[0]),flattened_target_tensor,:] = grad_output\n",
        "        return grad, zeros_like(target_token_tensor)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Lrzs03nSsBEE"
      },
      "outputs": [],
      "source": [
        "class TokenClsLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TokenClsLayer, self).__init__()\n",
        "        self.tcf = TokenClsFunction.apply\n",
        "        \n",
        "    def forward(self, features, token_indexes):        \n",
        "        return self.tcf(features,token_indexes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT Classifier**"
      ],
      "metadata": {
        "id": "MlBnAQVASztf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "benY5QnFsFUi"
      },
      "outputs": [],
      "source": [
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.5):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.config = BertConfig()\n",
        "        self.num_labels = 2\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
        "        self.tokenselectlayer = TokenClsLayer()\n",
        "        self.linear = nn.Linear(768, self.num_labels)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        nn.init.xavier_normal_(self.linear.weight)\n",
        "        #self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, input_id, mask,_target_token_ids):\n",
        "        _encoded_layers, pooled_output = self.bert(input_id, mask)\n",
        "        _target_token_embeddings = self.tokenselectlayer(_encoded_layers,_target_token_ids)\n",
        "        dropout_output = self.dropout(_target_token_embeddings)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        final_layer = self.softmax(linear_output)\n",
        "        return final_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "oMG62wnLsFbV"
      },
      "outputs": [],
      "source": [
        "df = df_train[['input_ids','sent_indexes','target_token_idx','is_proper_gloss']]\n",
        "\n",
        "train_df, val_df =  train_test_split(df, \n",
        "                                        random_state=None, \n",
        "                                        test_size=.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "XrnZzoxTsFgJ"
      },
      "outputs": [],
      "source": [
        "def train(model, train_data, val_data, learning_rate, epochs):\n",
        "\n",
        "    train, val = CorpusDataset(train_data), CorpusDataset(val_data)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
        "\n",
        "    if use_cuda:\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "    q = False\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "\n",
        "            for b_tokens_tensor, b_sentence_tensor, b_target_token_tensor,train_label  in tqdm(train_dataloader):\n",
        "                b_tokens_tensor=b_tokens_tensor.to(device)\n",
        "                b_sentence_tensor=b_sentence_tensor.to(device)\n",
        "                b_target_token_tensor=b_target_token_tensor.to(device)\n",
        "                train_label=train_label.to(device)\n",
        "\n",
        "                output = model(b_tokens_tensor, \n",
        "                               b_sentence_tensor, \n",
        "                               b_target_token_tensor)\n",
        "                \n",
        "                batch_loss = criterion(output, train_label)\n",
        "                total_loss_train += batch_loss.item()\n",
        "                \n",
        "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            total_acc_val = 0\n",
        "            total_loss_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_b_tokens_tensor, val_b_sentence_tensor, val_b_target_token_tensor,val_label in val_dataloader:\n",
        "\n",
        "                    val_b_tokens_tensor = val_b_tokens_tensor.to(device)\n",
        "                    val_b_sentence_tensor = val_b_sentence_tensor.to(device)\n",
        "                    val_b_target_token_tensor = val_b_target_token_tensor.to(device)\n",
        "                    val_label = val_label.to(device)\n",
        "                    \n",
        "\n",
        "                    output = model(val_b_tokens_tensor, val_b_sentence_tensor, val_b_target_token_tensor)\n",
        "\n",
        "                    batch_loss = criterion(output, val_label)\n",
        "                    total_loss_val += batch_loss.item()\n",
        "                    \n",
        "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "                    total_acc_val += acc\n",
        "            \n",
        "            print(\n",
        "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
        "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
        "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
        "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2S4AxO2NsFmG",
        "outputId": "efacf821-1e5c-4f3d-c41b-76ed91f25ea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 433/433 [00:00<00:00, 182105.05B/s]\n",
            "100%|██████████| 440473133/440473133 [00:08<00:00, 49826729.68B/s]\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 5\n",
        "model = BertClassifier()\n",
        "LR = 1e-6"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the Model**"
      ],
      "metadata": {
        "id": "xaO95bH_S6vd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "VmHBHu4Kr3Ly",
        "outputId": "df109050-619a-4cfd-b721-94ae6cb282db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 8/2262 [00:41<3:16:57,  5.24s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-ccc738615bee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-c5a90418892c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, val_data, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train(model, train_df, val_df, LR, EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE Model**"
      ],
      "metadata": {
        "id": "1QgAFyAoYsF5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XrZGlVjwQXE"
      },
      "outputs": [],
      "source": [
        "# create an iterator object with write permission - model.pkl\n",
        "import pickle\n",
        "modelsavepath='./model.pkl'\n",
        "with open(modelsavepath, 'wb') as files:\n",
        "    pickle.dump(model, files)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Bert_Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}