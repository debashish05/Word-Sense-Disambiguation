{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Necessary Packages**"
      ],
      "metadata": {
        "id": "31ISyZHan0ga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "YVYRsid0E9G5"
      },
      "outputs": [],
      "source": [
        "import os,glob,pathlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm,trange\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus.reader.wordnet import WordNetError\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import itertools\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense,Dropout,Bidirectional,Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "import itertools\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import random\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.utils import resample\n",
        "import nltk\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "from keras.callbacks import Callback,ModelCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Packages for Github cloning. Cloning the Dataset from Princton Repo**"
      ],
      "metadata": {
        "id": "YoC30gSrn8kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gitpython\n",
        "import git\n",
        "nltk.download('wordnet')\n",
        "repo = git.Repo.clone_from(\"https://github.com/rubenIzquierdo/wsd_corpora.git\", \"./data/raw/wsd_corpora\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjTS0saxoAtT",
        "outputId": "830016b7-72bb-4404-cf2d-c21f0d559461"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20 kB 12.9 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 30 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 40 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 51 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 181 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython) (4.2.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.9 gitpython-3.1.27 smmap-5.0.0\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting displaying options**"
      ],
      "metadata": {
        "id": "lIPJWuRLoDvU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uWSE7YPUFJ7B"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parsing The Semcor 3.0 Dataset**"
      ],
      "metadata": {
        "id": "kB4xqHTVoISe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C-fsyJqgHnmu"
      },
      "outputs": [],
      "source": [
        "def xml_parse(_fpath):\n",
        "\n",
        "    sctree = ET.parse(_fpath)\n",
        "\n",
        "    # Iterates over list of words in files    \n",
        "    dct_list1 = []\n",
        "    for node in sctree.iter('wf'):\n",
        "        attributes = node.attrib\n",
        "        attributes['text'] = node.text\n",
        "        dct_list1.append(attributes)\n",
        "\n",
        "    # Iterates over terms to find senses and corresponding sense references\n",
        "    dct_list2 = []\n",
        "    for term in sctree.iter('term'):\n",
        "        lemma = term.attrib.get('lemma')\n",
        "        wordid = term.find('span/target').attrib.get('id')\n",
        "        pos = ''\n",
        "\n",
        "        wnsn = '0'\n",
        "        senseid=''\n",
        "        if term.findall('externalReferences/externalRef'):\n",
        "            wnsn = term.findall('externalReferences/externalRef')[0].attrib.get('reference')\n",
        "            senseid = term.findall('externalReferences/externalRef')[1].attrib.get('reference')\n",
        "        dct_list2.append({'id':wordid,'lemma':lemma,'wn_sense_num':wnsn,'lexical_key':senseid,'pos':term.attrib['pos']})\n",
        "\n",
        "    word_df = pd.DataFrame(dct_list1)\n",
        "    sense_ref_df = pd.DataFrame(dct_list2)   \n",
        "    \n",
        "    return pd.merge(word_df,sense_ref_df,on='id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1jGUIuilHp9N"
      },
      "outputs": [],
      "source": [
        "def gen_file_list(_basepath,ext='*.naf'):\n",
        "    \n",
        "    file_list = []\n",
        "    fla = glob.glob(os.path.join(_basepath,ext))\n",
        "    flb = glob.glob(os.path.join(_basepath,'*',ext))\n",
        "    flc = glob.glob(os.path.join(_basepath,'**',ext))\n",
        "    files = set(fla+flb+flc)\n",
        "    for fileref in files: #search recursively for files\n",
        "        parent_folder_name = pathlib.Path(fileref).parent.name\n",
        "        file_name = pathlib.Path(fileref).name.split('.')[0]\n",
        "        \n",
        "        file_list.append( {'file_path':fileref,\n",
        "                           'parent_folder':parent_folder_name,\n",
        "                           'file_name':file_name})\n",
        "\n",
        "    return pd.DataFrame(file_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OuBPcEQCHq45"
      },
      "outputs": [],
      "source": [
        "def parse_corpus(_basepath,filter_validation = False):\n",
        "\n",
        "   # generate dataframe with references to all files\n",
        "    _fpath_df = gen_file_list(_basepath)\n",
        "    \n",
        "    # filter to remove validation files\n",
        "    filtered_file_df = _fpath_df\n",
        "    if filter_validation:\n",
        "         filtered_file_df = _fpath_df[_fpath_df.parent_folder != 'brownv']\n",
        "    \n",
        "    _dflist = []\n",
        "    for i,file_entry in tqdm(filtered_file_df.iterrows(), total=filtered_file_df.shape[0]):\n",
        "        _parsed_file_df = xml_parse(file_entry.file_path)\n",
        "        _parsed_file_df['file'] = file_entry.file_name\n",
        "        _dflist.append(_parsed_file_df)\n",
        "\n",
        "    return pd.concat(_dflist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vol1KLRhHuF8"
      },
      "outputs": [],
      "source": [
        "def build_corpus(_basepath,verbose=True,**kwargs):\n",
        "    if verbose: print('Parsing corpus')\n",
        "    base_corpus = parse_corpus(_basepath,**kwargs)\n",
        "\n",
        "    # Build wordnet ref key using wordnet lemma\n",
        "    if verbose: print('Preprocessing indexes...',end=\"\")\n",
        "    base_corpus['wn_index'] = base_corpus['lemma']+'%'+base_corpus['lexical_key']\n",
        "\n",
        "    base_corpus.loc[base_corpus.lexical_key == '','wn_index'] = ''\n",
        "    base_corpus.drop('lexical_key',axis=1,inplace=True)\n",
        "    if verbose: print('Done!')\n",
        "    return base_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Locznd4LHwOd"
      },
      "outputs": [],
      "source": [
        "def wordnet_get_glosses(_word,_sense_id):\n",
        "    _sense_id = int(_sense_id)\n",
        "    if not _word: # if ref is empty\n",
        "        return ''\n",
        "    try:\n",
        "        all_synsets = wn.synsets(_word)\n",
        "        target_gloss = []\n",
        "        other_glosses = []\n",
        "        for syn in all_synsets:\n",
        "            split = syn.name().split('.')\n",
        "            wn_lemma = split[0]\n",
        "            sense_num = int(split[-1])\n",
        "            if sense_num == _sense_id:\n",
        "                target_gloss.append(syn.definition()) \n",
        "            else:\n",
        "                other_glosses.append(syn.definition())                \n",
        "        return target_gloss,other_glosses\n",
        "    except (AttributeError,WordNetError,ValueError) as err:\n",
        "        return 'WN Error',None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NYrJNnw0HyPO"
      },
      "outputs": [],
      "source": [
        "def wordnet_gloss_helper(_word,_sense_id):\n",
        "    if not _word or not _sense_id:\n",
        "        return '',''\n",
        "    senseidlist = _sense_id.split(';')\n",
        "    if len(senseidlist) == 1:\n",
        "        return wordnet_get_glosses(_word,int(_sense_id))\n",
        "    elif len(senseidlist) > 1:\n",
        "        list_proper_glosses = []\n",
        "        other_gloss_set = set()\n",
        "        for senseid in senseidlist:\n",
        "            gloss, other_glosses =  wordnet_get_glosses(_word,int(senseid))\n",
        "            if gloss:\n",
        "                list_proper_glosses.append(gloss)\n",
        "                other_gloss_set.update(set(other_glosses))\n",
        "        # if one of the glosses is bogus return only one\n",
        "        if len(list_proper_glosses) == 1:\n",
        "            return list_proper_glosses[0], other_gloss_set\n",
        "        return list_proper_glosses, other_gloss_set\n",
        "    else:\n",
        "        return  'WN Error',[]   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5JY__AxCH0qv"
      },
      "outputs": [],
      "source": [
        "def add_wordnet_gloss(_semcordf,verbose=True):\n",
        "\n",
        "    if verbose: print('Adding wordnet glosses')\n",
        "    _semcordf['idx'] = list(range(len(_semcordf))) #adding index for merging\n",
        "    tqdm.pandas(desc=\"Gloss preprocessing\") \n",
        "    _glosses = _semcordf[_semcordf.wn_sense_num != '0'].progress_apply(lambda _row: (*wordnet_gloss_helper(_row['lemma'],_row['wn_sense_num'])\\\n",
        "                                                                        ,_row['idx']),axis=1 )\n",
        "    _df_glosses = pd.DataFrame(_glosses.values.tolist(),columns=['gloss','other_glosses','idx'])\n",
        "    _merged = pd.merge(_semcordf,_df_glosses,on='idx',how='left').fillna('')\n",
        "    # for now take only first gloss\n",
        "    _merged['gloss'] = _merged.gloss.apply(lambda x: x[0] if x else '')\n",
        "    # tag how many other glosses there are\n",
        "    _merged['other_glossesnum'] = _merged.other_glosses.apply(lambda x: len(x))   \n",
        "    if verbose: print('Done!')   \n",
        "    return _merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "D-F6aLwnH4TY"
      },
      "outputs": [],
      "source": [
        "def gen_sentence_context_pairs(_df):\n",
        "    sentence = _df.text.str.cat(sep = ' ').replace(\" '\",\"'\")\n",
        "    basedct = {'context':sentence,\n",
        "               'file':_df.iloc[0].file}\n",
        "\n",
        "    semcor_sentences = []\n",
        "\n",
        "    # Make sure there are other glosses and that the gloss column is not null\n",
        "    for i,(j,line) in enumerate(_df[(_df.other_glossesnum > 0) & (_df.gloss != 'WN Error') & (_df.gloss != '')].iterrows()): \n",
        "\n",
        "        newbasedct = basedct.copy()\n",
        "        newbasedct['target_word'] = line.text\n",
        "        newbasedct['gloss'] = line.gloss\n",
        "        newbasedct['is_proper_gloss'] = True\n",
        "        semcor_sentences.append(newbasedct)\n",
        "        # Then append all different contexes with False labels\n",
        "        for other_glosses in line.other_glosses:\n",
        "            newbasedct = basedct.copy()\n",
        "            newbasedct['target_word'] = line.text\n",
        "            newbasedct['gloss'] = other_glosses\n",
        "            newbasedct['is_proper_gloss'] = False\n",
        "            semcor_sentences.append(newbasedct)\n",
        "                \n",
        "    return semcor_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lU9nW34yH7pv"
      },
      "outputs": [],
      "source": [
        "def build_joint_dataset(_df):\n",
        "    groupbyobj = _df.groupby(['sent','file'])\n",
        "    full_dict_list = []\n",
        "    for [sentnum,file],gp in tqdm(groupbyobj,total=len(groupbyobj)):\n",
        "        full_dict_list.extend(gen_sentence_context_pairs(gp))\n",
        "    cols = ['file','context','target_word','gloss','is_proper_gloss']\n",
        "    return pd.DataFrame(full_dict_list)[cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8dEZnrcNH90u"
      },
      "outputs": [],
      "source": [
        "def build_corpus_dataset(_basepath,verbose=True,byref=False):\n",
        "    \n",
        "    corpus_df = build_corpus(_basepath,verbose=verbose)\n",
        "    corpus_df = add_wordnet_gloss(corpus_df,verbose=verbose)\n",
        "    if verbose: print('Processing adn labeling joint context-gloss pairs...',end=\"\")\n",
        "    final_corpus = build_joint_dataset(corpus_df)\n",
        "    if verbose: print('Done!')    \n",
        "    return final_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving the Final Corpus**"
      ],
      "metadata": {
        "id": "_lD5oMpRolTK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FLHRqrkiIDfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "404e2518-4dab-4e13-b69b-0012f430cffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing corpus\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:45<00:00,  7.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing indexes...Done!\n",
            "Adding wordnet glosses\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gloss preprocessing: 100%|██████████| 226040/226040 [00:27<00:00, 8241.63it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n",
            "Processing adn labeling joint context-gloss pairs..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 37168/37168 [01:57<00:00, 316.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "fpath = \"./data/raw/wsd_corpora/semcor3.0/\"\n",
        "os.mkdir(\"./data/raw/wsd_corpora/preprocessed\")\n",
        "savepath = r\"./data/raw/wsd_corpora/preprocessed/semcor_gloss.pkl\"\n",
        "final_corpus = build_corpus_dataset(fpath)\n",
        "final_corpus.to_pickle(savepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading the Final Saved Corpus**"
      ],
      "metadata": {
        "id": "2cR9EckxoqEa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OLfPrd3RsGPJ"
      },
      "outputs": [],
      "source": [
        "final_corpus = pd.read_pickle(savepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "A3hGIYCgO-Oy",
        "outputId": "c0ca1a60-74e6-47fa-8923-02d3cddab4bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     file  \\\n",
              "0  br-a01   \n",
              "1  br-a01   \n",
              "2  br-a01   \n",
              "3  br-a01   \n",
              "4  br-a01   \n",
              "\n",
              "                                                                                                                                                  context  \\\n",
              "0  The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place   \n",
              "1  The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place   \n",
              "2  The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place   \n",
              "3  The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place   \n",
              "4  The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place   \n",
              "\n",
              "                target_word  \\\n",
              "0  Fulton_County_Grand_Jury   \n",
              "1  Fulton_County_Grand_Jury   \n",
              "2  Fulton_County_Grand_Jury   \n",
              "3  Fulton_County_Grand_Jury   \n",
              "4                      said   \n",
              "\n",
              "                                                                                          gloss  \\\n",
              "0                                         any number of entities (members) considered as a unit   \n",
              "1  (chemistry) two or more atoms bound together as a single unit and forming part of a molecule   \n",
              "2   a set that is closed, associative, has an identity element and every element has an inverse   \n",
              "3                                                                form a group or group together   \n",
              "4                                                                           the chance to speak   \n",
              "\n",
              "   is_proper_gloss  \n",
              "0             True  \n",
              "1            False  \n",
              "2            False  \n",
              "3            False  \n",
              "4             True  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5d57ba56-f34a-4bec-952b-f1437bf208b2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file</th>\n",
              "      <th>context</th>\n",
              "      <th>target_word</th>\n",
              "      <th>gloss</th>\n",
              "      <th>is_proper_gloss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>br-a01</td>\n",
              "      <td>The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place</td>\n",
              "      <td>Fulton_County_Grand_Jury</td>\n",
              "      <td>any number of entities (members) considered as a unit</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>br-a01</td>\n",
              "      <td>The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place</td>\n",
              "      <td>Fulton_County_Grand_Jury</td>\n",
              "      <td>(chemistry) two or more atoms bound together as a single unit and forming part of a molecule</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>br-a01</td>\n",
              "      <td>The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place</td>\n",
              "      <td>Fulton_County_Grand_Jury</td>\n",
              "      <td>a set that is closed, associative, has an identity element and every element has an inverse</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>br-a01</td>\n",
              "      <td>The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place</td>\n",
              "      <td>Fulton_County_Grand_Jury</td>\n",
              "      <td>form a group or group together</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>br-a01</td>\n",
              "      <td>The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place</td>\n",
              "      <td>said</td>\n",
              "      <td>the chance to speak</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5d57ba56-f34a-4bec-952b-f1437bf208b2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5d57ba56-f34a-4bec-952b-f1437bf208b2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5d57ba56-f34a-4bec-952b-f1437bf208b2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "final_corpus.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FToJGHEMJU1c"
      },
      "outputs": [],
      "source": [
        "orig_final_corpus = final_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrfOmUgInom4",
        "outputId": "bb0e689a-137c-4f30-9c16-c53945db7468"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((178570, 5), (1411189, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "final_corpus[final_corpus.is_proper_gloss==True].shape,final_corpus[final_corpus.is_proper_gloss==False].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Under Sampling the data**"
      ],
      "metadata": {
        "id": "buLjQp_IoyYT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "BkzhtFxLnom5"
      },
      "outputs": [],
      "source": [
        "X_df, y_df  = final_corpus.iloc[:, :-1] , final_corpus.iloc[:, -1]\n",
        "\n",
        "under = RandomUnderSampler(sampling_strategy=0.4)\n",
        "X_df, y_df = under.fit_resample(X_df, y_df)\n",
        "\n",
        "over = RandomOverSampler(sampling_strategy=.5)\n",
        "X_df,y_df = over.fit_resample(X_df , y_df)\n",
        "\n",
        "X_df['is_proper_gloss'] = y_df\n",
        "final_corpus = X_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwuwtcWfnom6",
        "outputId": "3e45b60f-ab6a-429e-c4a5-f103b6925323"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((223212, 5), (446425, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "final_corpus[final_corpus.is_proper_gloss==True].shape,final_corpus[final_corpus.is_proper_gloss==False].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RVPyYbv2nom6"
      },
      "outputs": [],
      "source": [
        "final_corpus = final_corpus.sample(frac=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train and Test Split**"
      ],
      "metadata": {
        "id": "nxV-JI_So3CI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UPdrtwSOnom8"
      },
      "outputs": [],
      "source": [
        "train_df, val_df =  train_test_split(final_corpus, \n",
        "                                        random_state=None, \n",
        "                                        test_size=.1)\n",
        "val_df, test_df =  train_test_split(val_df, \n",
        "                                        random_state=None, \n",
        "                                        test_size=.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j87glEUjnom8",
        "outputId": "3d8e43f3-418f-4ff7-b229-fbf79b44811a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((602673, 5), (60267, 5), (6697, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "train_df.shape , val_df.shape, test_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loader**"
      ],
      "metadata": {
        "id": "xvQSSrLXo6W3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KUPsqCq4JXl_"
      },
      "outputs": [],
      "source": [
        "class MLDataGen(tf.keras.utils.Sequence):\n",
        "    \n",
        "    def __init__(self, df, batch_size = 64, gen_type=  'train', model_file_lang1 = None, model_file_lang2 = None):\n",
        "        self.context =list(map(lambda row:  ''.join(char for char in row[2] if char.isalnum())+' '+ row[1]+' '+ ''.join(char for char in row[2] if char.isalnum()),df.to_numpy())) \n",
        "        self.tagged_sense = list(map(lambda row: str(row[3]),df.to_numpy())) \n",
        "        self.label= df['is_proper_gloss'].astype(int).to_numpy()\n",
        "        self.gen_type = gen_type\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        self.max_seq_len=50\n",
        "        self.x0_vocab_size =0\n",
        "        self.x1_vocab_size =0\n",
        "        self.x0_tokenizer = None\n",
        "        self.x1_tokenizer = None\n",
        "        self._x0=[]\n",
        "        self._x1=[]\n",
        "        self._y=[]\n",
        "        self._initialize_variables()\n",
        "        self.model_file_lang1 = model_file_lang1\n",
        "        self.model_file_lang2 = model_file_lang2\n",
        "        #oov_tok = '<OOV>'\n",
        "\n",
        "\n",
        "    def _initialize_variables(self):         \n",
        "        \n",
        "        if self.gen_type == 'train':\n",
        "            tokenized = [line.split() for line in self.context]\n",
        "            flattendata = list(itertools.chain.from_iterable(tokenized))\n",
        "            word_freq = dict(Counter(flattendata))\n",
        "            tot_cnt  = len(word_freq)\n",
        "            #cnt = len([k for k,v in word_freq.items() if v >= 1])\n",
        "\n",
        "            # Prepare a tokenizer, again -- by not considering the rare words\n",
        "            self.x0_tokenizer = Tokenizer(num_words = tot_cnt+1 ,oov_token= '<OOV>')\n",
        "            # saving\n",
        "            with open('tokenizer_lang1.pickle', 'wb') as handle:\n",
        "                pickle.dump(self.x0_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        else:\n",
        "            #if self.model_file_lang1 != None: print('Please provide tokenizer model for lang1.')\n",
        "\n",
        "            # loading\n",
        "            with open('tokenizer_lang1.pickle', 'rb') as handle:\n",
        "                self.x0_tokenizer = pickle.load(handle)\n",
        "                \n",
        "            if self.x0_tokenizer  ==None:\n",
        "                print('Vocabulary is not set. Try training mode')\n",
        "                \n",
        "        self.x0_tokenizer.fit_on_texts(list(self.context))\n",
        "        self.x0_vocab_size = self.x0_tokenizer.num_words + 1\n",
        "        \n",
        "        # Convert text sequences to integer sequences \n",
        "        x0_train_seq = self.x0_tokenizer.texts_to_sequences(self.context) \n",
        "\n",
        "        # Pad zero upto maximum length\n",
        "        x0_train = pad_sequences(x0_train_seq,  maxlen=self.max_seq_len, padding='post')\n",
        "        \n",
        "        if self.gen_type == 'train':\n",
        "            tokenized = [line.strip().split() for line in self.tagged_sense]\n",
        "            flattendata = list(itertools.chain.from_iterable(tokenized))\n",
        "            word_freq = dict(Counter(flattendata))\n",
        "            tot_cnt  = len(word_freq)\n",
        "            #cnt = len([k for k,v in word_freq.items() if v >= 5])\n",
        "            # Prepare a tokenizer, again -- by not considering the rare words\n",
        "            self.x1_tokenizer = Tokenizer(num_words = tot_cnt ,oov_token= '<OOV>')\n",
        "            # saving\n",
        "            with open('tokenizer_lang2.pickle', 'wb') as handle:\n",
        "                pickle.dump(self.x1_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        else:\n",
        "            \n",
        "            #if self.model_file_lang2 != None: print('Please provide tokenizer model for lang2.')\n",
        "\n",
        "            # loading\n",
        "            with open('tokenizer_lang2.pickle', 'rb') as handle:\n",
        "                self.x1_tokenizer = pickle.load(handle)\n",
        "                \n",
        "            if self.x1_tokenizer  ==None:\n",
        "                print('Vocabulary is not set. Try training mode')\n",
        "\n",
        "\n",
        " \n",
        "        self.x1_tokenizer.fit_on_texts(list(self.tagged_sense))\n",
        "        self.x1_vocab_size = self.x1_tokenizer.num_words + 1\n",
        "        \n",
        "        # Convert text sequences to integer sequences \n",
        "        x1_train_seq = self.x1_tokenizer.texts_to_sequences(self.tagged_sense)\n",
        "\n",
        "        # Pad zero upto maximum length\n",
        "        x1_train = pad_sequences(x1_train_seq,  maxlen=self.max_seq_len, padding='post')\n",
        "\n",
        "        self._x0 = x0_train\n",
        "        self._x1 = x1_train\n",
        "        self._y = self.label\n",
        "    \n",
        "    \n",
        "        \n",
        "    def get_data_batch(self,i):           \n",
        "       \n",
        "        x0 = self._x0[i * self.batch_size:(i + 1) * self.batch_size]\n",
        "        x1 = self._x1[i * self.batch_size:(i + 1) * self.batch_size]       \n",
        "        y = self._y[i * self.batch_size:(i + 1) * self.batch_size].reshape(-1,1)#y.reshape(y.shape[0], y.shape[1], 1)[:, 1:]\n",
        "        return [np.array(x0), np.array(x1)],np.array(y)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        X, y = self.get_data_batch(index)\n",
        "        return X,y\n",
        "     \n",
        "    def __len__(self):\n",
        "        return len(self._x0) // self.batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation and Train Data**"
      ],
      "metadata": {
        "id": "m4png6RZo_OZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "D8a_rfCInom-"
      },
      "outputs": [],
      "source": [
        "data_gen = MLDataGen(train_df,batch_size=256)\n",
        "val_gen = MLDataGen(val_df, gen_type='val',batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Glove Embedding**"
      ],
      "metadata": {
        "id": "0egsVWutpEUm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEvz6lPlaNAU",
        "outputId": "3659b774-1112-4718-dff1-82fbbf82b2a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-01 17:24:29--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-05-01 17:24:29--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-05-01 17:24:29--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 40s  \n",
            "\n",
            "2022-05-01 17:27:10 (5.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embeddings of the data using Glove**"
      ],
      "metadata": {
        "id": "1cnWHGtTpJZp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg-yQnUDbIeR",
        "outputId": "da85c104-b900-4e20-daf4-f392b77d32f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "path_to_glove_file = './glove.6B.50d.txt'\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "GJO_WRuUcaMZ"
      },
      "outputs": [],
      "source": [
        "word_index_0 = data_gen.x0_tokenizer.word_index\n",
        "word_index_1 = data_gen.x1_tokenizer.word_index\n",
        "\n",
        "num_tokens_0 = (data_gen.x0_vocab_size) + 2\n",
        "num_tokens_1 = (data_gen.x1_vocab_size) + 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding Matrix**"
      ],
      "metadata": {
        "id": "D6_h_yIQpOrU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL8tZYyHbYZ5",
        "outputId": "be73bd84-4f35-48b0-d1ac-e75187b5ee92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 33178 words (6788 misses)\n"
          ]
        }
      ],
      "source": [
        "embedding_dim = 50\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix_0 = np.zeros((num_tokens_0, embedding_dim))\n",
        "for word, i in word_index_0.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_0[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyIs7GPtePJj",
        "outputId": "3c4ec69b-9d78-4ab8-8a1a-6fe9887f118b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 50261 words (7965 misses)\n"
          ]
        }
      ],
      "source": [
        "embedding_matrix_1 = np.zeros((num_tokens_1, embedding_dim))\n",
        "for word, i in word_index_1.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_1[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nueral Network Architecture**"
      ],
      "metadata": {
        "id": "aLBdHVBOpT62"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "OnQGRnZbnonR"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 50\n",
        "latent_dim = 64\n",
        "\n",
        "embedding_layer = Embedding(num_tokens_0,\n",
        "                            EMBEDDING_DIM,\n",
        "                            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix_0),\n",
        "                            trainable=False,\n",
        "                            mask_zero=True)\n",
        "\n",
        "encoder_inputs = Input(shape=(data_gen.max_seq_len,), name=\"encoder_input\")\n",
        "\n",
        "encoder_emb = embedding_layer(encoder_inputs)\n",
        "\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c = Bidirectional(\n",
        "                                                                    LSTM(latent_dim,\n",
        "                                                                         return_sequences=True, \n",
        "                                                                         return_state=True, \n",
        "                                                                         dropout=0.1,\n",
        "                                                                         recurrent_dropout=0.1), \n",
        "                                                                    merge_mode=\"concat\", \n",
        "                                                                    name=\"encoder_lstm_1\")(encoder_emb)\n",
        "encoder_states = [forward_h, forward_c, backward_h, backward_c]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYnS1BT9nonS",
        "outputId": "a01f4a75-3c87-4b93-bd27-fa64415cdcaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_input (InputLayer)     [(None, 50)]         0           []                               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 50, 50)       2823950     ['encoder_input[0][0]']          \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 50)     1209800     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " encoder_lstm_1 (Bidirectional)  [(None, 50, 128),   58880       ['embedding[0][0]']              \n",
            "                                 (None, 64),                                                      \n",
            "                                 (None, 64),                                                      \n",
            "                                 (None, 64),                                                      \n",
            "                                 (None, 64)]                                                      \n",
            "                                                                                                  \n",
            " decoder_lstm_1 (Bidirectional)  (None, 128)         58880       ['embedding_1[0][0]',            \n",
            "                                                                  'encoder_lstm_1[0][1]',         \n",
            "                                                                  'encoder_lstm_1[0][2]',         \n",
            "                                                                  'encoder_lstm_1[0][3]',         \n",
            "                                                                  'encoder_lstm_1[0][4]']         \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 64)           8256        ['decoder_lstm_1[0][0]']         \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            65          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,159,831\n",
            "Trainable params: 126,081\n",
            "Non-trainable params: 4,033,750\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "decoder_inputs = Input(shape=(None, ))\n",
        "\n",
        "dec_emb_layer = Embedding(num_tokens_1 , \n",
        "                          embedding_dim, \n",
        "                          embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix_1),\n",
        "                          trainable=False,\n",
        "                          mask_zero=True)\n",
        "\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm =  Bidirectional(\n",
        "    LSTM(latent_dim, dropout=0.1,recurrent_dropout=0.1), \n",
        "    merge_mode=\"concat\", name=\"decoder_lstm_1\")\n",
        "\n",
        "decoder_outputs = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "decoder_dense1 = Dense(64, activation='relu')\n",
        "decoder_outputs = decoder_dense1(decoder_outputs)\n",
        "decoder_dense2 = Dense(1, activation='sigmoid')\n",
        "decoder_outputs = decoder_dense2(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**F1 Score Calculation**"
      ],
      "metadata": {
        "id": "fhHPum1BpYpQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ePtJ4u3h5oAW"
      },
      "outputs": [],
      "source": [
        "def get_score(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compiling and Saving checkpoints. Also added Early Stopping**"
      ],
      "metadata": {
        "id": "SaaoP3veplfU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "QdqUwzw3r3BW"
      },
      "outputs": [],
      "source": [
        "adam = Adam(learning_rate=0.01)\n",
        "model.compile(   loss=tf.keras.losses.BinaryCrossentropy(), \n",
        "                optimizer=adam, \n",
        "                metrics=[get_score,'accuracy'])\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
        "\n",
        "checkpoint_path = \"./data/raw/wsd_corpora/preprocessed/checkpoints/cp-{epoch:04d}.ckpt\"\n",
        "\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "   checkpoint_path, verbose=1, save_weights_only=True,\n",
        "   # Save weights, every epoch.\n",
        "   save_freq='epoch')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fitting the Model**"
      ],
      "metadata": {
        "id": "_nJrmyeap41D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKIEO_RxnonW",
        "outputId": "0ddd136f-3cba-443d-fbaa-91c2ecb240de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2354/2354 [==============================] - ETA: 0s - loss: 0.3982 - get_score: 0.6971 - accuracy: 0.8101\n",
            "Epoch 1: saving model to ./data/preprocessed/checkpoints\\cp-0001.ckpt\n",
            "2354/2354 [==============================] - 5940s 3s/step - loss: 0.3982 - get_score: 0.6971 - accuracy: 0.8101 - val_loss: 0.6513 - val_get_score: 0.3675 - val_accuracy: 0.6826\n",
            "Epoch 2/10\n",
            "2354/2354 [==============================] - ETA: 0s - loss: 0.3528 - get_score: 0.7512 - accuracy: 0.8388\n",
            "Epoch 2: saving model to ./data/preprocessed/checkpoints\\cp-0002.ckpt\n",
            "2354/2354 [==============================] - 7598s 3s/step - loss: 0.3528 - get_score: 0.7512 - accuracy: 0.8388 - val_loss: 0.6872 - val_get_score: 0.3794 - val_accuracy: 0.6844\n",
            "Epoch 3/10\n",
            "2354/2354 [==============================] - ETA: 0s - loss: 0.3460 - get_score: 0.7570 - accuracy: 0.8424\n",
            "Epoch 3: saving model to ./data/preprocessed/checkpoints\\cp-0003.ckpt\n",
            "2354/2354 [==============================] - 6778s 3s/step - loss: 0.3460 - get_score: 0.7570 - accuracy: 0.8424 - val_loss: 0.6918 - val_get_score: 0.3782 - val_accuracy: 0.6769\n",
            "Epoch 3: early stopping\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    data_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=10,\n",
        "    callbacks=[es,cp_callback],\n",
        "    verbose=1\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "akA3IOinr2yc"
      },
      "outputs": [],
      "source": [
        "reverse_target_word_index = data_gen.x0_tokenizer.index_word\n",
        "reverse_source_word_index = data_gen.x0_tokenizer.index_word\n",
        "target_word_index = data_gen.x1_tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSetJqRxnonZ"
      },
      "outputs": [],
      "source": [
        "# Inference Models\n",
        "\n",
        "# Encode the input sequence to get the feature vector\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs,\n",
        "                      forward_h, forward_c, backward_h, backward_c])\n",
        "\n",
        "# Decoder setup\n",
        "\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_forward_h = Input(shape=(latent_dim, ))\n",
        "decoder_state_input_forward_c = Input(shape=(latent_dim, ))\n",
        "decoder_state_input_backward_h = Input(shape=(latent_dim, ))\n",
        "decoder_state_input_backward_c = Input(shape=(latent_dim, ))\n",
        "\n",
        "decoder_hidden_state_input = Input(shape=(data_gen.max_seq_len, ))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2 = decoder_lstm(dec_emb2,\n",
        "        initial_state=[decoder_state_input_forward_h, decoder_state_input_forward_c,decoder_state_input_backward_h,decoder_state_input_backward_c])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense1(decoder_outputs2)\n",
        "decoder_outputs2 = decoder_dense2(decoder_outputs2)\n",
        "\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,\n",
        "                      decoder_state_input_forward_h, decoder_state_input_forward_c,decoder_state_input_backward_h,decoder_state_input_backward_c],\n",
        "                      [decoder_outputs2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJbIhEAtnonZ"
      },
      "outputs": [],
      "source": [
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wn.ADJ,\n",
        "                \"N\": wn.NOUN,\n",
        "                \"V\": wn.VERB,\n",
        "                \"R\": wn.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wn.NOUN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sense Prediction**"
      ],
      "metadata": {
        "id": "fn3QZotVrnWi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CF2Qj0snona"
      },
      "outputs": [],
      "source": [
        "def predictSense(sentence, target):\n",
        "    targets=[]\n",
        "    if '_' in target:\n",
        "        targets = target.split('_')\n",
        "    else:\n",
        "        targets=[target]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for t in targets:\n",
        "        lemmas.append(lemmatizer.lemmatize(t, get_wordnet_pos(t)))\n",
        "    lemma = '_'.join(lemmas)\n",
        "    all_synsets = wn.synsets(lemma)\n",
        "    glosses = []\n",
        "    for syn in all_synsets:\n",
        "        glosses.append(syn.definition())\n",
        "    \n",
        "    if len(glosses)==0:\n",
        "        return ''    \n",
        "    \n",
        "    x0_test_seq = data_gen.x0_tokenizer.texts_to_sequences([sentence+' '+target]*len(glosses))\n",
        "    x0_test = pad_sequences(x0_test_seq,  maxlen=data_gen.max_seq_len, padding='post')\n",
        "\n",
        "    x1_test_seq = data_gen.x1_tokenizer.texts_to_sequences(glosses)\n",
        "    x1_test = pad_sequences(x1_test_seq,  maxlen=data_gen.max_seq_len, padding='post')\n",
        "    \n",
        "    #return x0_test , x1_test    \n",
        "    try:\n",
        "        (e_out, e1_h, e1_c, e2_h, e2_c) = encoder_model.predict(x0_test)\n",
        "        ypred = decoder_model.predict([x1_test] + [e_out, e1_h, e1_c, e2_h, e2_c])\n",
        "    except:\n",
        "        print('error ', lemma,targets)\n",
        "    return glosses[np.argmax(ypred)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Set Data preprations**"
      ],
      "metadata": {
        "id": "VFEo3yrXrqDH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vmv9ZoxPnonb"
      },
      "outputs": [],
      "source": [
        "def build_pred_dataset(_df):\n",
        "    context_target = _df[['context','target_word']].drop_duplicates()\n",
        "    print(context_target.shape,_df.shape)\n",
        "    basedct = {}\n",
        "    predictions = []\n",
        "    context_target['predicted_gloss'] = context_target[['context','target_word']].apply(lambda row: predictSense(row[0],row[1]),axis=1)\n",
        "    prediction_df = pd.merge(_df,context_target,on=['context','target_word'],how='right').fillna('')\n",
        "    prediction_df['Predicitons'] = prediction_df[['gloss','predicted_gloss']].apply(lambda row: True if row[0] == row[1] else False,axis=1)\n",
        "    \n",
        "    return prediction_df.drop(columns=['predicted_gloss'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results**"
      ],
      "metadata": {
        "id": "yQi8bYiRrwM4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHGZJQAPnonc",
        "outputId": "4bc9f3f2-badf-4979-e2ac-c18790e97130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(100, 2) (100, 5)\n"
          ]
        }
      ],
      "source": [
        "pred = build_pred_dataset(test_df[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFGxrsYrnonc"
      },
      "outputs": [],
      "source": [
        "pred_df = pred[pred.is_proper_gloss==True]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuywyJRvnone",
        "outputId": "873b517b-48e9-4337-b2fc-7ad5ea89e8a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(29, 6)"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ-OZBKBnonf",
        "outputId": "f6d9773b-c572-4f69-f8e7-5555a8f36f59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7, 6)"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_df[pred_df.is_proper_gloss ==pred_df.Predicitons].shape"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BiLSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}