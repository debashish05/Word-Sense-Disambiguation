{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQyl7lppInq1",
    "outputId": "5552f27e-4480-4a0a-bde3-5f951bee0333"
   },
   "outputs": [],
   "source": [
    "#!pip install gitpython\n",
    "#!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YVYRsid0E9G5",
    "outputId": "d942778f-0ba3-4e62-d14b-aefb5d21a702"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rajapal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,glob,pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import git\n",
    "import shutil\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm,trange\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import WordNetError\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tokenize the text to get the vocab count \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense,Dropout,Bidirectional, \\\n",
    "    Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import resample\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uWSE7YPUFJ7B"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2WzYKfAJFGot"
   },
   "outputs": [],
   "source": [
    "#repo = git.Repo.clone_from(\"https://github.com/rubenIzquierdo/wsd_corpora.git\", \"./notebooks/data/raw/wsd_corpora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2wTl5e3HHiN8"
   },
   "outputs": [],
   "source": [
    "#shutil.copytree(r\"./notebooks/data/raw/wsd_corpora/semcor3.0\", r\"./notebooks/data/raw/semcor3.0\")\n",
    "#shutil.copytree(r\"./notebooks/data/raw/wsd_corpora/semeval2007_task17_allwords\", r\"./notebooks/data/raw/semeval2007_task17_allwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "C-fsyJqgHnmu"
   },
   "outputs": [],
   "source": [
    "def xml_parse(_fpath):\n",
    "\n",
    "    sctree = ET.parse(_fpath)\n",
    "\n",
    "    # Iterates over list of words in files    \n",
    "    dct_list1 = []\n",
    "    for node in sctree.iter('wf'):\n",
    "        attributes = node.attrib\n",
    "        attributes['text'] = node.text\n",
    "        dct_list1.append(attributes)\n",
    "\n",
    "    # Iterates over terms to find senses and corresponding sense references\n",
    "    dct_list2 = []\n",
    "    for term in sctree.iter('term'):\n",
    "        lemma = term.attrib.get('lemma')\n",
    "        wordid = term.find('span/target').attrib.get('id')\n",
    "        pos = ''\n",
    "\n",
    "        wnsn = '0'\n",
    "        senseid=''\n",
    "        if term.findall('externalReferences/externalRef'):\n",
    "            wnsn = term.findall('externalReferences/externalRef')[0].attrib.get('reference')\n",
    "            senseid = term.findall('externalReferences/externalRef')[1].attrib.get('reference')\n",
    "        dct_list2.append({'id':wordid,'lemma':lemma,'wn_sense_num':wnsn,'lexical_key':senseid,'pos':term.attrib['pos']})\n",
    "\n",
    "    word_df = pd.DataFrame(dct_list1)\n",
    "    sense_ref_df = pd.DataFrame(dct_list2)   \n",
    "    \n",
    "    return pd.merge(word_df,sense_ref_df,on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1jGUIuilHp9N"
   },
   "outputs": [],
   "source": [
    "def gen_file_list(_basepath,ext='*.naf'):\n",
    "    \n",
    "    file_list = []\n",
    "    fla = glob.glob(os.path.join(_basepath,ext))\n",
    "    flb = glob.glob(os.path.join(_basepath,'*',ext))\n",
    "    flc = glob.glob(os.path.join(_basepath,'**',ext))\n",
    "    files = set(fla+flb+flc)\n",
    "    for fileref in files: #search recursively for files\n",
    "        parent_folder_name = pathlib.Path(fileref).parent.name\n",
    "        file_name = pathlib.Path(fileref).name.split('.')[0]\n",
    "        \n",
    "        file_list.append( {'file_path':fileref,\n",
    "                           'parent_folder':parent_folder_name,\n",
    "                           'file_name':file_name})\n",
    "\n",
    "    return pd.DataFrame(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OuBPcEQCHq45"
   },
   "outputs": [],
   "source": [
    "def parse_corpus(_basepath,filter_validation = False):\n",
    "\n",
    "   # generate dataframe with references to all files\n",
    "    _fpath_df = gen_file_list(_basepath)\n",
    "    \n",
    "    # filter to remove validation files\n",
    "    filtered_file_df = _fpath_df\n",
    "    if filter_validation:\n",
    "         filtered_file_df = _fpath_df[_fpath_df.parent_folder != 'brownv']\n",
    "    \n",
    "    _dflist = []\n",
    "    for i,file_entry in tqdm(filtered_file_df.iterrows(), total=filtered_file_df.shape[0]):\n",
    "        _parsed_file_df = xml_parse(file_entry.file_path)\n",
    "        #print(_parsed_file_df.head())\n",
    "        _parsed_file_df['file'] = file_entry.file_name\n",
    "        _dflist.append(_parsed_file_df)\n",
    "\n",
    "    return pd.concat(_dflist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vol1KLRhHuF8"
   },
   "outputs": [],
   "source": [
    "def build_corpus(_basepath,verbose=True,**kwargs):\n",
    "    if verbose: print('Parsing corpus')\n",
    "    base_corpus = parse_corpus(_basepath,**kwargs)\n",
    "\n",
    "    # Build wordnet ref key using wordnet lemma\n",
    "    if verbose: print('Preprocessing indexes...',end=\"\")\n",
    "    base_corpus['wn_index'] = base_corpus['lemma']+'%'+base_corpus['lexical_key']\n",
    "\n",
    "    base_corpus.loc[base_corpus.lexical_key == '','wn_index'] = ''\n",
    "    base_corpus.drop('lexical_key',axis=1,inplace=True)\n",
    "    if verbose: print('Done!')\n",
    "    return base_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"./data/raw/semcor3.0/\"\n",
    "#corpus = build_corpus(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Locznd4LHwOd"
   },
   "outputs": [],
   "source": [
    "def wordnet_get_glosses(_word,_sense_id):\n",
    "    _sense_id = int(_sense_id)\n",
    "    if not _word: # if ref is empty\n",
    "        return ''\n",
    "    try:\n",
    "        all_synsets = wn.synsets(_word)\n",
    "        target_gloss = []\n",
    "        other_glosses = []\n",
    "        for syn in all_synsets:\n",
    "            split = syn.name().split('.')\n",
    "            wn_lemma = split[0]\n",
    "            sense_num = int(split[-1])\n",
    "            #if _word == wn_lemma:    \n",
    "            if sense_num == _sense_id:\n",
    "                target_gloss.append(syn.definition()) \n",
    "            else:\n",
    "                other_glosses.append(syn.definition())                \n",
    "        return target_gloss,other_glosses\n",
    "    except (AttributeError,WordNetError,ValueError) as err:\n",
    "        return 'WN Error',None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NYrJNnw0HyPO"
   },
   "outputs": [],
   "source": [
    "def wordnet_gloss_helper(_word,_sense_id):\n",
    "    if not _word or not _sense_id:\n",
    "        return '',''\n",
    "    senseidlist = _sense_id.split(';')\n",
    "    if len(senseidlist) == 1:\n",
    "        return wordnet_get_glosses(_word,int(_sense_id))\n",
    "    elif len(senseidlist) > 1:\n",
    "        list_proper_glosses = []\n",
    "        other_gloss_set = set()\n",
    "        for senseid in senseidlist:\n",
    "            gloss, other_glosses =  wordnet_get_glosses(_word,int(senseid))\n",
    "            if gloss:\n",
    "                list_proper_glosses.append(gloss)\n",
    "                other_gloss_set.update(set(other_glosses))\n",
    "        # if one of the glosses is bogus return only one\n",
    "        if len(list_proper_glosses) == 1:\n",
    "            return list_proper_glosses[0], other_gloss_set\n",
    "        #print(list_proper_glosses, other_gloss_set)\n",
    "        return list_proper_glosses, other_gloss_set\n",
    "    else:\n",
    "        return  'WN Error',[]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5JY__AxCH0qv"
   },
   "outputs": [],
   "source": [
    "def add_wordnet_gloss(_semcordf,verbose=True):\n",
    "\n",
    "    if verbose: print('Adding wordnet glosses')\n",
    "    _semcordf['idx'] = list(range(len(_semcordf))) #adding index for merging\n",
    "    tqdm.pandas(desc=\"Gloss preprocessing\") \n",
    "    _glosses = _semcordf[_semcordf.wn_sense_num != '0'].progress_apply(lambda _row: (*wordnet_gloss_helper(_row['lemma'],_row['wn_sense_num'])\\\n",
    "                                                                        ,_row['idx']),axis=1 )\n",
    "    _df_glosses = pd.DataFrame(_glosses.values.tolist(),columns=['gloss','other_glosses','idx'])\n",
    "    _merged = pd.merge(_semcordf,_df_glosses,on='idx',how='left').fillna('')\n",
    "    # for now take only first gloss\n",
    "    _merged['gloss'] = _merged.gloss.apply(lambda x: x[0] if x else '')\n",
    "    # tag how many other glosses there are\n",
    "    _merged['other_glossesnum'] = _merged.other_glosses.apply(lambda x: len(x))   \n",
    "    if verbose: print('Done!')   \n",
    "    return _merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "D-F6aLwnH4TY"
   },
   "outputs": [],
   "source": [
    "def gen_sentence_context_pairs(_df):\n",
    "    sentence = _df.text.str.cat(sep = ' ').replace(\" '\",\"'\")#_df.text.tolist()\n",
    "    basedct = {'context':sentence,\n",
    "               'file':_df.iloc[0].file}\n",
    "\n",
    "    semcor_sentences = []\n",
    "\n",
    "    # Make sure there are other glosses and that the gloss column is not null\n",
    "    for i,(j,line) in enumerate(_df[(_df.other_glossesnum > 0) & (_df.gloss != 'WN Error') & (_df.gloss != '')].iterrows()): \n",
    "\n",
    "        newbasedct = basedct.copy()\n",
    "        #newbasedct['context'] = ' '.join(sentence[max(0,i-3): min(i+3,len(sentence))])\n",
    "        newbasedct['target_word'] = line.text\n",
    "        newbasedct['gloss'] = line.gloss\n",
    "        newbasedct['is_proper_gloss'] = True\n",
    "        semcor_sentences.append(newbasedct)\n",
    "        # Then append all different contexes with False labels\n",
    "        for other_glosses in line.other_glosses:\n",
    "            newbasedct = basedct.copy()\n",
    "            #newbasedct['context'] = ' '.join(sentence[max(0,i-3): min(i+3,len(sentence))])\n",
    "            newbasedct['target_word'] = line.text\n",
    "            newbasedct['gloss'] = other_glosses\n",
    "            newbasedct['is_proper_gloss'] = False\n",
    "            semcor_sentences.append(newbasedct)\n",
    "                \n",
    "    return semcor_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lU9nW34yH7pv"
   },
   "outputs": [],
   "source": [
    "def build_joint_dataset(_df):\n",
    "    groupbyobj = _df.groupby(['sent','file'])\n",
    "    full_dict_list = []\n",
    "    for [sentnum,file],gp in tqdm(groupbyobj,total=len(groupbyobj)):\n",
    "        full_dict_list.extend(gen_sentence_context_pairs(gp))\n",
    "    cols = ['file','context','target_word','gloss','is_proper_gloss']\n",
    "    return pd.DataFrame(full_dict_list)[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8dEZnrcNH90u"
   },
   "outputs": [],
   "source": [
    "def build_corpus_dataset(_basepath,verbose=True,byref=False):\n",
    "    \n",
    "    corpus_df = build_corpus(_basepath,verbose=verbose)\n",
    "    corpus_df = add_wordnet_gloss(corpus_df,verbose=verbose)\n",
    "    if verbose: print('Processing adn labeling joint context-gloss pairs...',end=\"\")\n",
    "    final_corpus = build_joint_dataset(corpus_df)\n",
    "    if verbose: print('Done!')    \n",
    "    return final_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "FLHRqrkiIDfA"
   },
   "outputs": [],
   "source": [
    "fpath = \"./data/raw/semcor3.0/\"\n",
    "savepath = r\"./data/preprocessed/semcor_gloss.pkl\"\n",
    "#final_corpus = build_corpus_dataset(fpath)\n",
    "#final_corpus.to_pickle(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "OLfPrd3RsGPJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "final_corpus = pd.read_pickle(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "A3hGIYCgO-Oy",
    "outputId": "f598afe9-46fc-44f4-b1c5-2af696f18a03"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>context</th>\n",
       "      <th>target_word</th>\n",
       "      <th>gloss</th>\n",
       "      <th>is_proper_gloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>br-a01</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place</td>\n",
       "      <td>Fulton_County_Grand_Jury</td>\n",
       "      <td>any number of entities (members) considered as a unit</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>br-a01</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place</td>\n",
       "      <td>Fulton_County_Grand_Jury</td>\n",
       "      <td>(chemistry) two or more atoms bound together as a single unit and forming part of a molecule</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>br-a01</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place</td>\n",
       "      <td>Fulton_County_Grand_Jury</td>\n",
       "      <td>a set that is closed, associative, has an identity element and every element has an inverse</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>br-a01</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place</td>\n",
       "      <td>Fulton_County_Grand_Jury</td>\n",
       "      <td>form a group or group together</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>br-a01</td>\n",
       "      <td>The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place</td>\n",
       "      <td>said</td>\n",
       "      <td>the chance to speak</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     file  \\\n",
       "0  br-a01   \n",
       "1  br-a01   \n",
       "2  br-a01   \n",
       "3  br-a01   \n",
       "4  br-a01   \n",
       "\n",
       "                                                                                                                                                  context  \\\n",
       "0  The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place   \n",
       "1  The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place   \n",
       "2  The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place   \n",
       "3  The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place   \n",
       "4  The Fulton_County_Grand_Jury said Friday an investigation of Atlanta's recent primary_election produced no evidence that any irregularities took_place   \n",
       "\n",
       "                target_word  \\\n",
       "0  Fulton_County_Grand_Jury   \n",
       "1  Fulton_County_Grand_Jury   \n",
       "2  Fulton_County_Grand_Jury   \n",
       "3  Fulton_County_Grand_Jury   \n",
       "4                      said   \n",
       "\n",
       "                                                                                          gloss  \\\n",
       "0                                         any number of entities (members) considered as a unit   \n",
       "1  (chemistry) two or more atoms bound together as a single unit and forming part of a molecule   \n",
       "2   a set that is closed, associative, has an identity element and every element has an inverse   \n",
       "3                                                                form a group or group together   \n",
       "4                                                                           the chance to speak   \n",
       "\n",
       "   is_proper_gloss  \n",
       "0             True  \n",
       "1            False  \n",
       "2            False  \n",
       "3            False  \n",
       "4             True  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "FToJGHEMJU1c"
   },
   "outputs": [],
   "source": [
    "orig_final_corpus = final_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_KpSYPE6SATJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1589759, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_final_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((178570, 5), (1411189, 5))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_corpus[final_corpus.is_proper_gloss==True].shape,final_corpus[final_corpus.is_proper_gloss==False].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df, y_df  = final_corpus.iloc[:, :-1] , final_corpus.iloc[:, -1]\n",
    "\n",
    "under = RandomUnderSampler(sampling_strategy=0.4)\n",
    "X_df, y_df = under.fit_resample(X_df, y_df)\n",
    "\n",
    "over = RandomOverSampler(sampling_strategy=.5)\n",
    "X_df,y_df = over.fit_resample(X_df , y_df)\n",
    "\n",
    "X_df['is_proper_gloss'] = y_df\n",
    "final_corpus = X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((223212, 5), (446425, 5))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_corpus[final_corpus.is_proper_gloss==True].shape,final_corpus[final_corpus.is_proper_gloss==False].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus = final_corpus.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df =  train_test_split(final_corpus, \n",
    "                                        random_state=None, \n",
    "                                        test_size=.1)\n",
    "val_df, test_df =  train_test_split(val_df, \n",
    "                                        random_state=None, \n",
    "                                        test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((602673, 5), (60267, 5), (6697, 5))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape , val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "KUPsqCq4JXl_"
   },
   "outputs": [],
   "source": [
    "class MLDataGen(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, df, batch_size = 64, gen_type=  'train', model_file_lang1 = None, model_file_lang2 = None):\n",
    "        self.context =list(map(lambda row:  ''.join(char for char in row[2] if char.isalnum())+' '+ row[1]+' '+ ''.join(char for char in row[2] if char.isalnum()),df.to_numpy())) \n",
    "        self.tagged_sense = list(map(lambda row: str(row[3]),df.to_numpy())) \n",
    "        self.label= df['is_proper_gloss'].astype(int).to_numpy()\n",
    "        self.gen_type = gen_type\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.max_seq_len=50\n",
    "        self.x0_vocab_size =0\n",
    "        self.x1_vocab_size =0\n",
    "        self.x0_tokenizer = None\n",
    "        self.x1_tokenizer = None\n",
    "        self._x0=[]\n",
    "        self._x1=[]\n",
    "        self._y=[]\n",
    "        self._initialize_variables()\n",
    "        self.model_file_lang1 = model_file_lang1\n",
    "        self.model_file_lang2 = model_file_lang2\n",
    "        #oov_tok = '<OOV>'\n",
    "\n",
    "\n",
    "    def _initialize_variables(self):         \n",
    "        \n",
    "        if self.gen_type == 'train':\n",
    "            tokenized = [line.split() for line in self.context]\n",
    "            flattendata = list(itertools.chain.from_iterable(tokenized))\n",
    "            word_freq = dict(Counter(flattendata))\n",
    "            tot_cnt  = len(word_freq)\n",
    "            #cnt = len([k for k,v in word_freq.items() if v >= 1])\n",
    "\n",
    "            # Prepare a tokenizer, again -- by not considering the rare words\n",
    "            self.x0_tokenizer = Tokenizer(num_words = tot_cnt+1 ,oov_token= '<OOV>')\n",
    "            # saving\n",
    "            with open('tokenizer_lang1.pickle', 'wb') as handle:\n",
    "                pickle.dump(self.x0_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        else:\n",
    "            #if self.model_file_lang1 != None: print('Please provide tokenizer model for lang1.')\n",
    "\n",
    "            # loading\n",
    "            with open('tokenizer_lang1.pickle', 'rb') as handle:\n",
    "                self.x0_tokenizer = pickle.load(handle)\n",
    "                \n",
    "            if self.x0_tokenizer  ==None:\n",
    "                print('Vocabulary is not set. Try training mode')\n",
    "                \n",
    "        self.x0_tokenizer.fit_on_texts(list(self.context))\n",
    "        self.x0_vocab_size = self.x0_tokenizer.num_words + 1\n",
    "        \n",
    "        # Convert text sequences to integer sequences \n",
    "        x0_train_seq = self.x0_tokenizer.texts_to_sequences(self.context) \n",
    "\n",
    "        # Pad zero upto maximum length\n",
    "        x0_train = pad_sequences(x0_train_seq,  maxlen=self.max_seq_len, padding='post')\n",
    "        \n",
    "        if self.gen_type == 'train':\n",
    "            tokenized = [line.strip().split() for line in self.tagged_sense]\n",
    "            flattendata = list(itertools.chain.from_iterable(tokenized))\n",
    "            word_freq = dict(Counter(flattendata))\n",
    "            tot_cnt  = len(word_freq)\n",
    "            #cnt = len([k for k,v in word_freq.items() if v >= 5])\n",
    "            # Prepare a tokenizer, again -- by not considering the rare words\n",
    "            self.x1_tokenizer = Tokenizer(num_words = tot_cnt ,oov_token= '<OOV>')\n",
    "            # saving\n",
    "            with open('tokenizer_lang2.pickle', 'wb') as handle:\n",
    "                pickle.dump(self.x1_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        else:\n",
    "            \n",
    "            #if self.model_file_lang2 != None: print('Please provide tokenizer model for lang2.')\n",
    "\n",
    "            # loading\n",
    "            with open('tokenizer_lang2.pickle', 'rb') as handle:\n",
    "                self.x1_tokenizer = pickle.load(handle)\n",
    "                \n",
    "            if self.x1_tokenizer  ==None:\n",
    "                print('Vocabulary is not set. Try training mode')\n",
    "\n",
    "\n",
    " \n",
    "        self.x1_tokenizer.fit_on_texts(list(self.tagged_sense))\n",
    "        self.x1_vocab_size = self.x1_tokenizer.num_words + 1\n",
    "        \n",
    "        # Convert text sequences to integer sequences \n",
    "        x1_train_seq = self.x1_tokenizer.texts_to_sequences(self.tagged_sense)\n",
    "\n",
    "        # Pad zero upto maximum length\n",
    "        x1_train = pad_sequences(x1_train_seq,  maxlen=self.max_seq_len, padding='post')\n",
    "\n",
    "        self._x0 = x0_train\n",
    "        self._x1 = x1_train\n",
    "        self._y = self.label\n",
    "    \n",
    "    \n",
    "        \n",
    "    def get_data_batch(self,i):           \n",
    "       \n",
    "        x0 = self._x0[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "        x1 = self._x1[i * self.batch_size:(i + 1) * self.batch_size]       \n",
    "        y = self._y[i * self.batch_size:(i + 1) * self.batch_size].reshape(-1,1)#y.reshape(y.shape[0], y.shape[1], 1)[:, 1:]\n",
    "        return [np.array(x0), np.array(x1)],np.array(y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X, y = self.get_data_batch(index)\n",
    "        return X,y\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self._x0) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = MLDataGen(train_df,batch_size=256)\n",
    "val_gen = MLDataGen(val_df, gen_type='val',batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZEvz6lPlaNAU",
    "outputId": "c6627e40-744e-4f98-ccd4-df974c27ce50"
   },
   "outputs": [],
   "source": [
    "#wget.download('http://nlp.stanford.edu/data/glove.6B.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import zipfile as zf\n",
    "#files = zf.ZipFile(\"glove.6B.zip'\",'r')\n",
    "#files.extractall()\n",
    "#files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rg-yQnUDbIeR",
    "outputId": "6ac96811-780e-4a1b-c62c-2590e4e238aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_glove_file = './data/glove.6B.50d.txt'\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "GJO_WRuUcaMZ"
   },
   "outputs": [],
   "source": [
    "word_index_0 = data_gen.x0_tokenizer.word_index\n",
    "word_index_1 = data_gen.x1_tokenizer.word_index\n",
    "\n",
    "num_tokens_0 = (data_gen.x0_vocab_size) + 2\n",
    "num_tokens_1 = (data_gen.x1_vocab_size) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qL8tZYyHbYZ5",
    "outputId": "938f14dc-fb3e-4dbb-a16b-1509197d03d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 33181 words (6800 misses)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix_0 = np.zeros((num_tokens_0, embedding_dim))\n",
    "for word, i in word_index_0.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_0[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jyIs7GPtePJj",
    "outputId": "ce189f5e-047d-49d5-d14f-55455f94251d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 50416 words (7996 misses)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_1 = np.zeros((num_tokens_1, embedding_dim))\n",
    "for word, i in word_index_1.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_1[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "latent_dim = 64\n",
    "\n",
    "embedding_layer = Embedding(num_tokens_0,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix_0),\n",
    "                            trainable=False,\n",
    "                            mask_zero=True)\n",
    "\n",
    "encoder_inputs = Input(shape=(data_gen.max_seq_len,), name=\"encoder_input\")\n",
    "\n",
    "encoder_emb = embedding_layer(encoder_inputs)\n",
    "\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = Bidirectional(\n",
    "                                                                    LSTM(latent_dim,\n",
    "                                                                         return_sequences=True, \n",
    "                                                                         return_state=True, \n",
    "                                                                         dropout=0.1,\n",
    "                                                                         recurrent_dropout=0.1), \n",
    "                                                                    merge_mode=\"concat\", \n",
    "                                                                    name=\"encoder_lstm_1\")(encoder_emb)\n",
    "encoder_states = [forward_h, forward_c, backward_h, backward_c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 50, 50)       2824200     ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, None, 50)     1222050     ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " encoder_lstm_1 (Bidirectional)  [(None, 50, 128),   58880       ['embedding[0][0]']              \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " decoder_lstm_1 (Bidirectional)  (None, 128)         58880       ['embedding_3[0][0]',            \n",
      "                                                                  'encoder_lstm_1[0][1]',         \n",
      "                                                                  'encoder_lstm_1[0][2]',         \n",
      "                                                                  'encoder_lstm_1[0][3]',         \n",
      "                                                                  'encoder_lstm_1[0][4]']         \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64)           8256        ['decoder_lstm_1[0][0]']         \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            65          ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,172,331\n",
      "Trainable params: 126,081\n",
      "Non-trainable params: 4,046,250\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_inputs = Input(shape=(None, ))\n",
    "\n",
    "dec_emb_layer = Embedding(num_tokens_1 , \n",
    "                          embedding_dim, \n",
    "                          embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix_1),\n",
    "                          trainable=False,\n",
    "                          mask_zero=True)\n",
    "\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm =  Bidirectional(\n",
    "    LSTM(latent_dim, dropout=0.1,recurrent_dropout=0.1), \n",
    "    merge_mode=\"concat\", name=\"decoder_lstm_1\")\n",
    "\n",
    "decoder_outputs = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense1 = Dense(64, activation='relu')\n",
    "decoder_outputs = decoder_dense1(decoder_outputs)\n",
    "decoder_dense2 = Dense(1, activation='sigmoid')\n",
    "decoder_outputs = decoder_dense2(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qva1jyvN_vfW"
   },
   "outputs": [],
   "source": [
    "#round(1.6 * data_gen.x0_vocab_size+1 ** .56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "ePtJ4u3h5oAW"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def get_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    \n",
    "    #return true_positives/(possible_positives+K.epsilon()) \n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "QdqUwzw3r3BW"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "\n",
    "adam = Adam(learning_rate=0.01)\n",
    "model.compile(   loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "                optimizer=adam, \n",
    "                metrics=[get_score,'accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "\n",
    "checkpoint_path = \"./data/preprocessed/checkpoints/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "   checkpoint_path, verbose=1, save_weights_only=True,\n",
    "   # Save weights, every epoch.\n",
    "   save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2354/2354 [==============================] - ETA: 0s - loss: 0.3982 - get_score: 0.6971 - accuracy: 0.8101\n",
      "Epoch 1: saving model to ./data/preprocessed/checkpoints\\cp-0001.ckpt\n",
      "2354/2354 [==============================] - 5940s 3s/step - loss: 0.3982 - get_score: 0.6971 - accuracy: 0.8101 - val_loss: 0.6513 - val_get_score: 0.3675 - val_accuracy: 0.6826\n",
      "Epoch 2/10\n",
      "2354/2354 [==============================] - ETA: 0s - loss: 0.3528 - get_score: 0.7512 - accuracy: 0.8388\n",
      "Epoch 2: saving model to ./data/preprocessed/checkpoints\\cp-0002.ckpt\n",
      "2354/2354 [==============================] - 7598s 3s/step - loss: 0.3528 - get_score: 0.7512 - accuracy: 0.8388 - val_loss: 0.6872 - val_get_score: 0.3794 - val_accuracy: 0.6844\n",
      "Epoch 3/10\n",
      "2354/2354 [==============================] - ETA: 0s - loss: 0.3460 - get_score: 0.7570 - accuracy: 0.8424\n",
      "Epoch 3: saving model to ./data/preprocessed/checkpoints\\cp-0003.ckpt\n",
      "2354/2354 [==============================] - 6778s 3s/step - loss: 0.3460 - get_score: 0.7570 - accuracy: 0.8424 - val_loss: 0.6918 - val_get_score: 0.3782 - val_accuracy: 0.6769\n",
      "Epoch 3: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    data_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,\n",
    "    callbacks=[es,cp_callback],\n",
    "    verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYPNfdjS5SEa"
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import KFold,RepeatedStratifiedKFold\n",
    "#kf = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iW9iOochKARk",
    "outputId": "384dd6df-fb85-4599-d0c8-6dcdbdbb7653",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for train_index, test_index in rskf.split(final_corpus, np.zeros(len(final_corpus))):\n",
    "#  train_df=final_corpus.iloc[train_index]\n",
    "#  test_df = final_corpus.iloc[test_index]\n",
    "    #val_gen = MLDataGen(val_df, gen_type='val',batch_size=32)\n",
    "#  history = seq2seq_Model.fit(\n",
    "#      MLDataGen(train_df, gen_type='t',batch_size=128),\n",
    "#      validation_data=MLDataGen(test_df, gen_type='v',batch_size=128),\n",
    "#      verbose=1 ,\n",
    "#      epochs=5,\n",
    "#      callbacks = [cp_callback]\n",
    "#      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "akA3IOinr2yc"
   },
   "outputs": [],
   "source": [
    "reverse_target_word_index = data_gen.x0_tokenizer.index_word\n",
    "reverse_source_word_index = data_gen.x0_tokenizer.index_word\n",
    "target_word_index = data_gen.x1_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Models\n",
    "\n",
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs,\n",
    "                      forward_h, forward_c, backward_h, backward_c])\n",
    "\n",
    "# Decoder setup\n",
    "\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_forward_h = Input(shape=(latent_dim, ))\n",
    "decoder_state_input_forward_c = Input(shape=(latent_dim, ))\n",
    "decoder_state_input_backward_h = Input(shape=(latent_dim, ))\n",
    "decoder_state_input_backward_c = Input(shape=(latent_dim, ))\n",
    "\n",
    "decoder_hidden_state_input = Input(shape=(data_gen.max_seq_len, ))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2 = decoder_lstm(dec_emb2,\n",
    "        initial_state=[decoder_state_input_forward_h, decoder_state_input_forward_c,decoder_state_input_backward_h,decoder_state_input_backward_c])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense1(decoder_outputs2)\n",
    "decoder_outputs2 = decoder_dense2(decoder_outputs2)\n",
    "\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,\n",
    "                      decoder_state_input_forward_h, decoder_state_input_forward_c,decoder_state_input_backward_h,decoder_state_input_backward_c],\n",
    "                      [decoder_outputs2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wn.ADJ,\n",
    "                \"N\": wn.NOUN,\n",
    "                \"V\": wn.VERB,\n",
    "                \"R\": wn.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wn.NOUN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSense(sentence, target):\n",
    "    targets=[]\n",
    "    if '_' in target:\n",
    "        targets = target.split('_')\n",
    "    else:\n",
    "        targets=[target]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for t in targets:\n",
    "        lemmas.append(lemmatizer.lemmatize(t, get_wordnet_pos(t)))\n",
    "    lemma = '_'.join(lemmas)\n",
    "    all_synsets = wn.synsets(lemma)\n",
    "    glosses = []\n",
    "    for syn in all_synsets:\n",
    "        glosses.append(syn.definition())\n",
    "    \n",
    "    if len(glosses)==0:\n",
    "        return ''    \n",
    "    \n",
    "    x0_test_seq = data_gen.x0_tokenizer.texts_to_sequences([sentence+' '+target]*len(glosses))\n",
    "    x0_test = pad_sequences(x0_test_seq,  maxlen=data_gen.max_seq_len, padding='post')\n",
    "\n",
    "    x1_test_seq = data_gen.x1_tokenizer.texts_to_sequences(glosses)\n",
    "    x1_test = pad_sequences(x1_test_seq,  maxlen=data_gen.max_seq_len, padding='post')\n",
    "    \n",
    "    #return x0_test , x1_test    \n",
    "    try:\n",
    "        (e_out, e1_h, e1_c, e2_h, e2_c) = encoder_model.predict(x0_test)\n",
    "        ypred = decoder_model.predict([x1_test] + [e_out, e1_h, e1_c, e2_h, e2_c])\n",
    "    except:\n",
    "        print('error ', lemma,targets)\n",
    "    return glosses[np.argmax(ypred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pred_dataset(_df):\n",
    "    context_target = _df[['context','target_word']].drop_duplicates()\n",
    "    print(context_target.shape,_df.shape)\n",
    "    basedct = {}\n",
    "    predictions = []\n",
    "    context_target['predicted_gloss'] = context_target[['context','target_word']].apply(lambda row: predictSense(row[0],row[1]),axis=1)\n",
    "    prediction_df = pd.merge(_df,context_target,on=['context','target_word'],how='right').fillna('')\n",
    "    prediction_df['Predicitons'] = prediction_df[['gloss','predicted_gloss']].apply(lambda row: True if row[0] == row[1] else False,axis=1)\n",
    "    \n",
    "    return prediction_df.drop(columns=['predicted_gloss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (100, 5)\n"
     ]
    }
   ],
   "source": [
    "pred = build_pred_dataset(test_df[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred[pred.is_proper_gloss==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 6)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 6)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df[pred_df.is_proper_gloss ==pred_df.Predicitons].shape"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP WSD.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
